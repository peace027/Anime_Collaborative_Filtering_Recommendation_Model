{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a302a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, metrics, preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584b86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#torch.cuda.is_available()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f077b90c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rating_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3a495c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57633278 entries, 0 to 57633277\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype\n",
      "---  ------    -----\n",
      " 0   user_id   int64\n",
      " 1   anime_id  int64\n",
      " 2   rating    int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52502a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id     0\n",
       "anime_id    0\n",
       "rating      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5878970d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.763328e+07\n",
       "mean     1.768878e+05\n",
       "std      1.020117e+05\n",
       "min      0.000000e+00\n",
       "25%      8.827800e+04\n",
       "50%      1.772910e+05\n",
       "75%      2.654190e+05\n",
       "max      3.534040e+05\n",
       "Name: user_id, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1a431a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.763328e+07\n",
       "mean     1.583147e+04\n",
       "std      1.326114e+04\n",
       "min      1.000000e+00\n",
       "25%      3.091000e+03\n",
       "50%      1.188700e+04\n",
       "75%      2.899900e+04\n",
       "max      4.845600e+04\n",
       "Name: anime_id, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['anime_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ab3668",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310059"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5bf1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16872"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.anime_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3348825a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     14642156\n",
       "7     13325549\n",
       "9      9773857\n",
       "6      6849293\n",
       "10     6716048\n",
       "5      3436250\n",
       "4      1455102\n",
       "3       696048\n",
       "2       405556\n",
       "1       333419\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.value_counts() #check value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "651678ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57633278, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97deefe",
   "metadata": {},
   "source": [
    "#### Training Dataset Class Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2352624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AnimeDataset:\n",
    "    def __init__(self, users, animes, ratings):\n",
    "        self.users = users\n",
    "        self.animes = animes\n",
    "        self.ratings = ratings\n",
    "    # len(movie_dataset)\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    # movie_dataset[1] \n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        users = self.users[item] \n",
    "        animes = self.animes[item]\n",
    "        ratings = self.ratings[item]\n",
    "        \n",
    "        return {\n",
    "            \"users\": torch.tensor(users, dtype=torch.long),\n",
    "            \"animes\": torch.tensor(animes, dtype=torch.long),\n",
    "            \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd99fc",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55129d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RecSysModel(nn.Module):\n",
    "    def __init__(self, n_users, n_animes):\n",
    "        super().__init__()\n",
    "        # trainable lookup matrix for shallow embedding vectors\n",
    "        \n",
    "        self.user_embed = nn.Embedding(n_users, 32)\n",
    "        self.anime_embed = nn.Embedding(n_animes, 32)\n",
    "        # user, movie embedding concat\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, users, animes, ratings=None):\n",
    "        user_embeds = self.user_embed(users)\n",
    "        anime_embeds = self.anime_embed(animes)\n",
    "        output = torch.cat([user_embeds, anime_embeds], dim=1)\n",
    "        \n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bc38ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode the user and movie id to start from 0 so we don't run into index out of bound with Embedding\n",
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_anime = preprocessing.LabelEncoder()\n",
    "df.user_id = lbl_user.fit_transform(df.user_id.values)\n",
    "df.anime_id = lbl_anime.fit_transform(df.anime_id.values)\n",
    "\n",
    "df_train, df_valid = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df.rating.values\n",
    ")\n",
    "\n",
    "train_dataset = AnimeDataset(\n",
    "    users=df_train.user_id.values,\n",
    "    animes=df_train.anime_id.values,\n",
    "    ratings=df_train.rating.values\n",
    ")\n",
    "\n",
    "valid_dataset = AnimeDataset(\n",
    "    users=df_valid.user_id.values,\n",
    "    animes=df_valid.anime_id.values,\n",
    "    ratings=df_valid.rating.values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "015a031b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'users': tensor(211007), 'animes': tensor(13830), 'ratings': tensor(7)}\n",
      "{'users': tensor(123627), 'animes': tensor(3532), 'ratings': tensor(9)}\n",
      "{'users': tensor(40441), 'animes': tensor(9151), 'ratings': tensor(1)}\n",
      "{'users': tensor(251116), 'animes': tensor(20), 'ratings': tensor(9)}\n",
      "{'users': tensor(85875), 'animes': tensor(7254), 'ratings': tensor(5)}\n",
      "51869950\n",
      "5763328\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):  # Print the first 5 data points\n",
    "    data_point = train_dataset[i]\n",
    "    print(data_point)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c4c8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'users': tensor([ 58097, 227042, 295765, 163845]), 'animes': tensor([11583,  1571,  7383,    31]), 'ratings': tensor([ 8,  9, 10,  5])}\n",
      "12967488\n",
      "1440832\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=4,\n",
    "                          shuffle=True)\n",
    "\n",
    "validation_loader = DataLoader(dataset=valid_dataset,\n",
    "                               batch_size=4,\n",
    "                               shuffle=True)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "dataloader_data = next(dataiter)\n",
    "print(dataloader_data)\n",
    "print(len(train_loader))\n",
    "print(len(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "730488b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecSysModel(\n",
    "    n_users=len(lbl_user.classes_),\n",
    "    n_animes=len(lbl_anime.classes_),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())  \n",
    "sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e92dba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310059\n",
      "16872\n",
      "16871\n",
      "51869950\n"
     ]
    }
   ],
   "source": [
    "print(len(lbl_user.classes_))\n",
    "print(len(lbl_anime.classes_))\n",
    "print(df.anime_id.max())\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea3a2a",
   "metadata": {},
   "source": [
    "#### Manually run a forward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0b886f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 58097, 227042, 295765, 163845])\n",
      "torch.Size([4])\n",
      "tensor([11583,  1571,  7383,    31])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(dataloader_data['users'])\n",
    "\n",
    "print(dataloader_data['users'].size())\n",
    "print(dataloader_data['animes'] )\n",
    "print(dataloader_data['animes'].size())\n",
    "\n",
    "user_embed = nn.Embedding(len(lbl_user.classes_), 32)\n",
    "anime_embed = nn.Embedding(len(lbl_anime.classes_), 32)\n",
    "\n",
    "out = nn.Linear(64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a790d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_embeds torch.Size([4, 32])\n",
      "user_embeds tensor([[ 7.7243e-01, -1.5965e+00, -6.0004e-01,  5.0266e-01, -1.0823e+00,\n",
      "         -1.1506e+00,  8.5844e-01,  8.4515e-01,  7.3376e-01, -2.9686e-01,\n",
      "         -1.1021e+00,  3.6744e-02, -1.3023e+00, -4.6652e-01,  5.8638e-01,\n",
      "          1.2709e-01,  6.8724e-01,  1.2631e-01, -7.3367e-01,  7.6715e-01,\n",
      "          7.6379e-01, -5.8569e-01, -1.0590e+00,  1.2386e-01,  1.6112e+00,\n",
      "         -2.8967e-01,  1.0036e+00, -8.9182e-01, -1.3622e-01, -2.3797e-01,\n",
      "          2.9906e-02, -4.5136e-01],\n",
      "        [-1.5184e-01, -1.7212e+00,  6.7305e-01,  3.6604e-01,  6.2090e-01,\n",
      "          6.3496e-01,  2.9518e-01, -1.4230e+00, -1.8566e+00,  2.4161e+00,\n",
      "         -2.7963e-01,  6.9170e-01, -7.8318e-01, -1.3372e+00,  6.4404e-01,\n",
      "          1.1244e+00, -1.0874e+00, -1.9663e+00, -6.1607e-01, -2.1759e+00,\n",
      "         -3.7866e-01,  1.8794e+00, -1.8901e-01, -1.1275e+00,  1.5761e-01,\n",
      "          1.8646e+00,  7.7471e-01, -1.7129e+00, -8.9713e-01, -8.6602e-01,\n",
      "          1.4229e+00,  4.1028e-01],\n",
      "        [ 7.8093e-01,  8.0434e-01, -1.7880e+00,  4.6871e-01,  3.5784e-02,\n",
      "         -7.9631e-01,  1.6541e+00,  1.9219e+00, -9.5301e-01, -1.3262e+00,\n",
      "         -9.8559e-01,  1.6195e+00,  2.8895e+00, -1.5683e+00, -5.2747e-01,\n",
      "          6.6998e-01, -1.2992e+00,  1.5373e-02, -4.9099e-01,  3.1398e-01,\n",
      "          3.8525e-01, -4.0808e-01, -8.0130e-01,  1.0946e-01, -1.2233e+00,\n",
      "         -8.8836e-01,  3.5246e-02, -1.0687e+00,  6.5787e-01,  1.2472e-02,\n",
      "         -5.9513e-01, -3.0378e-01],\n",
      "        [ 8.0567e-01, -1.2460e+00, -1.8478e+00, -1.3925e+00,  9.0515e-01,\n",
      "          1.9877e-01, -1.8680e-01, -5.0011e-01, -1.2096e+00,  4.0446e-01,\n",
      "          1.6699e+00, -1.4712e+00,  1.1129e+00, -5.2897e-01,  1.0941e+00,\n",
      "         -9.7817e-01,  8.6270e-01,  1.0532e+00, -6.7947e-01,  4.9144e-01,\n",
      "         -8.5322e-01, -4.3904e-03, -8.5919e-01,  4.6668e-01,  4.5503e-01,\n",
      "          7.3478e-01,  7.0626e-01,  1.2871e+00,  1.8174e-01, -4.2597e-01,\n",
      "          1.1573e-03,  1.4703e+00]], grad_fn=<EmbeddingBackward0>)\n",
      "anime_embeds torch.Size([4, 32])\n",
      "anime_embeds tensor([[-1.1740e+00, -3.8761e-01,  6.6059e-01, -7.4245e-01,  3.8504e-01,\n",
      "          2.5229e+00, -7.4707e-01,  2.0519e+00,  4.9144e-02, -1.1749e+00,\n",
      "          1.0898e+00,  8.6778e-01, -6.7003e-01,  1.7910e+00,  7.7294e-01,\n",
      "          5.2404e-01, -5.2329e-01,  7.3376e-01,  3.8422e-01, -7.0036e-01,\n",
      "         -1.1896e+00, -1.5485e+00,  6.1016e-01, -7.1759e-01,  1.4432e+00,\n",
      "          1.1927e+00,  1.1731e+00, -1.1786e+00,  9.8466e-01, -2.5896e-02,\n",
      "         -1.3171e+00, -1.0724e+00],\n",
      "        [-1.6233e+00, -1.4761e+00,  7.6494e-01, -1.8772e+00, -1.0004e+00,\n",
      "          7.0248e-01, -7.0633e-01, -3.8227e-01, -6.9887e-01,  1.6043e-01,\n",
      "          1.5412e+00,  3.5417e-01, -9.3834e-01, -1.1470e+00, -1.8258e+00,\n",
      "         -2.6556e+00,  8.6530e-01,  7.7786e-01, -3.0085e-01, -5.5738e-01,\n",
      "         -2.5665e+00, -5.6156e-01,  1.2399e+00,  9.6242e-01,  7.8012e-01,\n",
      "          6.1776e-02,  1.0976e+00,  8.1889e-01, -2.1895e-01, -1.8553e+00,\n",
      "          2.5015e+00,  1.7847e-01],\n",
      "        [-4.0457e-01, -7.7927e-01, -1.7413e+00, -1.2480e+00, -1.0524e+00,\n",
      "         -1.8092e+00, -3.7369e-01, -1.0322e+00, -2.5118e-01,  1.0159e+00,\n",
      "          1.7288e+00, -1.2049e+00,  3.8949e-01, -1.2717e+00,  1.4504e+00,\n",
      "         -1.0211e+00,  2.3810e+00, -2.3651e+00, -1.2703e+00,  3.4156e-01,\n",
      "         -9.5437e-01,  1.5703e+00,  9.4657e-02,  4.5408e-01,  1.5045e+00,\n",
      "         -2.7770e-01, -4.0741e-01, -3.1297e-01, -2.8754e-01, -1.5227e+00,\n",
      "         -1.0256e-01, -8.3205e-01],\n",
      "        [ 7.1163e-01, -5.7179e-01,  2.4182e+00, -1.9776e+00,  4.8959e-01,\n",
      "          1.8661e+00,  1.9715e+00,  1.4013e-03, -5.7887e-01, -3.0317e-01,\n",
      "          4.9319e-01,  4.5181e-01, -1.1726e+00,  1.0888e+00, -6.5376e-01,\n",
      "          1.1110e+00, -4.1269e-01,  2.9371e-01, -1.4695e+00, -9.5331e-01,\n",
      "         -1.8242e+00, -2.1655e+00,  1.4582e-02,  7.8870e-01,  1.1611e+00,\n",
      "         -2.3420e-01, -4.8773e-01, -6.1671e-02, -8.9182e-01, -2.2762e-01,\n",
      "          1.2824e+00,  4.2610e-01]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "user_embeds = user_embed(dataloader_data['users'])\n",
    "anime_embeds = anime_embed(dataloader_data['animes'])\n",
    "print(f\"user_embeds {user_embeds.size()}\")\n",
    "print(f\"user_embeds {user_embeds}\")\n",
    "print(f\"anime_embeds {anime_embeds.size()}\")\n",
    "print(f\"anime_embeds {anime_embeds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1efb25cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([4, 64])\n",
      "output: tensor([[ 7.7243e-01, -1.5965e+00, -6.0004e-01,  5.0266e-01, -1.0823e+00,\n",
      "         -1.1506e+00,  8.5844e-01,  8.4515e-01,  7.3376e-01, -2.9686e-01,\n",
      "         -1.1021e+00,  3.6744e-02, -1.3023e+00, -4.6652e-01,  5.8638e-01,\n",
      "          1.2709e-01,  6.8724e-01,  1.2631e-01, -7.3367e-01,  7.6715e-01,\n",
      "          7.6379e-01, -5.8569e-01, -1.0590e+00,  1.2386e-01,  1.6112e+00,\n",
      "         -2.8967e-01,  1.0036e+00, -8.9182e-01, -1.3622e-01, -2.3797e-01,\n",
      "          2.9906e-02, -4.5136e-01, -1.1740e+00, -3.8761e-01,  6.6059e-01,\n",
      "         -7.4245e-01,  3.8504e-01,  2.5229e+00, -7.4707e-01,  2.0519e+00,\n",
      "          4.9144e-02, -1.1749e+00,  1.0898e+00,  8.6778e-01, -6.7003e-01,\n",
      "          1.7910e+00,  7.7294e-01,  5.2404e-01, -5.2329e-01,  7.3376e-01,\n",
      "          3.8422e-01, -7.0036e-01, -1.1896e+00, -1.5485e+00,  6.1016e-01,\n",
      "         -7.1759e-01,  1.4432e+00,  1.1927e+00,  1.1731e+00, -1.1786e+00,\n",
      "          9.8466e-01, -2.5896e-02, -1.3171e+00, -1.0724e+00],\n",
      "        [-1.5184e-01, -1.7212e+00,  6.7305e-01,  3.6604e-01,  6.2090e-01,\n",
      "          6.3496e-01,  2.9518e-01, -1.4230e+00, -1.8566e+00,  2.4161e+00,\n",
      "         -2.7963e-01,  6.9170e-01, -7.8318e-01, -1.3372e+00,  6.4404e-01,\n",
      "          1.1244e+00, -1.0874e+00, -1.9663e+00, -6.1607e-01, -2.1759e+00,\n",
      "         -3.7866e-01,  1.8794e+00, -1.8901e-01, -1.1275e+00,  1.5761e-01,\n",
      "          1.8646e+00,  7.7471e-01, -1.7129e+00, -8.9713e-01, -8.6602e-01,\n",
      "          1.4229e+00,  4.1028e-01, -1.6233e+00, -1.4761e+00,  7.6494e-01,\n",
      "         -1.8772e+00, -1.0004e+00,  7.0248e-01, -7.0633e-01, -3.8227e-01,\n",
      "         -6.9887e-01,  1.6043e-01,  1.5412e+00,  3.5417e-01, -9.3834e-01,\n",
      "         -1.1470e+00, -1.8258e+00, -2.6556e+00,  8.6530e-01,  7.7786e-01,\n",
      "         -3.0085e-01, -5.5738e-01, -2.5665e+00, -5.6156e-01,  1.2399e+00,\n",
      "          9.6242e-01,  7.8012e-01,  6.1776e-02,  1.0976e+00,  8.1889e-01,\n",
      "         -2.1895e-01, -1.8553e+00,  2.5015e+00,  1.7847e-01],\n",
      "        [ 7.8093e-01,  8.0434e-01, -1.7880e+00,  4.6871e-01,  3.5784e-02,\n",
      "         -7.9631e-01,  1.6541e+00,  1.9219e+00, -9.5301e-01, -1.3262e+00,\n",
      "         -9.8559e-01,  1.6195e+00,  2.8895e+00, -1.5683e+00, -5.2747e-01,\n",
      "          6.6998e-01, -1.2992e+00,  1.5373e-02, -4.9099e-01,  3.1398e-01,\n",
      "          3.8525e-01, -4.0808e-01, -8.0130e-01,  1.0946e-01, -1.2233e+00,\n",
      "         -8.8836e-01,  3.5246e-02, -1.0687e+00,  6.5787e-01,  1.2472e-02,\n",
      "         -5.9513e-01, -3.0378e-01, -4.0457e-01, -7.7927e-01, -1.7413e+00,\n",
      "         -1.2480e+00, -1.0524e+00, -1.8092e+00, -3.7369e-01, -1.0322e+00,\n",
      "         -2.5118e-01,  1.0159e+00,  1.7288e+00, -1.2049e+00,  3.8949e-01,\n",
      "         -1.2717e+00,  1.4504e+00, -1.0211e+00,  2.3810e+00, -2.3651e+00,\n",
      "         -1.2703e+00,  3.4156e-01, -9.5437e-01,  1.5703e+00,  9.4657e-02,\n",
      "          4.5408e-01,  1.5045e+00, -2.7770e-01, -4.0741e-01, -3.1297e-01,\n",
      "         -2.8754e-01, -1.5227e+00, -1.0256e-01, -8.3205e-01],\n",
      "        [ 8.0567e-01, -1.2460e+00, -1.8478e+00, -1.3925e+00,  9.0515e-01,\n",
      "          1.9877e-01, -1.8680e-01, -5.0011e-01, -1.2096e+00,  4.0446e-01,\n",
      "          1.6699e+00, -1.4712e+00,  1.1129e+00, -5.2897e-01,  1.0941e+00,\n",
      "         -9.7817e-01,  8.6270e-01,  1.0532e+00, -6.7947e-01,  4.9144e-01,\n",
      "         -8.5322e-01, -4.3904e-03, -8.5919e-01,  4.6668e-01,  4.5503e-01,\n",
      "          7.3478e-01,  7.0626e-01,  1.2871e+00,  1.8174e-01, -4.2597e-01,\n",
      "          1.1573e-03,  1.4703e+00,  7.1163e-01, -5.7179e-01,  2.4182e+00,\n",
      "         -1.9776e+00,  4.8959e-01,  1.8661e+00,  1.9715e+00,  1.4013e-03,\n",
      "         -5.7887e-01, -3.0317e-01,  4.9319e-01,  4.5181e-01, -1.1726e+00,\n",
      "          1.0888e+00, -6.5376e-01,  1.1110e+00, -4.1269e-01,  2.9371e-01,\n",
      "         -1.4695e+00, -9.5331e-01, -1.8242e+00, -2.1655e+00,  1.4582e-02,\n",
      "          7.8870e-01,  1.1611e+00, -2.3420e-01, -4.8773e-01, -6.1671e-02,\n",
      "         -8.9182e-01, -2.2762e-01,  1.2824e+00,  4.2610e-01]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "output: tensor([[ 0.6835],\n",
      "        [ 0.1593],\n",
      "        [-0.2346],\n",
      "        [ 0.7040]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = torch.cat([user_embeds, anime_embeds], dim=1) \n",
    "print(f\"output: {output.size()}\")\n",
    "print(f\"output: {output}\")\n",
    "output = out(output)\n",
    "print(f\"output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7484b3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_output: tensor([[ 0.7031],\n",
      "        [-0.5308],\n",
      "        [-0.5884],\n",
      "        [-0.4143]], device='cuda:0'), size: torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a GPU available (cuda:0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Move the input tensors to the GPU\n",
    "dataloader_data['users'] = dataloader_data['users'].to(device)\n",
    "dataloader_data['animes'] = dataloader_data['animes'].to(device)\n",
    "\n",
    "# Now you can use the model and input tensors together without any device mismatch error\n",
    "with torch.no_grad():\n",
    "    model_output = model(dataloader_data['users'], dataloader_data['animes'])\n",
    "    print(f\"model_output: {model_output}, size: {model_output.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4de1f40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8,  9, 10,  5])\n",
      "tensor([[ 8],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [ 5]])\n",
      "tensor([[ 0.7031],\n",
      "        [-0.5308],\n",
      "        [-0.5884],\n",
      "        [-0.4143]], device='cuda:0')\n",
      "tensor(32)\n",
      "tensor(-32.8303, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "rating = dataloader_data[\"ratings\"]\n",
    "print(rating)\n",
    "print(rating.view(4, -1))\n",
    "print(model_output)\n",
    "\n",
    "print(rating.sum())\n",
    "\n",
    "print(model_output.sum() - rating.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b84796",
   "metadata": {},
   "source": [
    "#### Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss at step: 5000 is 3.023882845926285\n",
      "epoch 0 loss at step: 10000 is 1.9771920445680617\n",
      "epoch 0 loss at step: 15000 is 1.1762241766512394\n",
      "epoch 0 loss at step: 20000 is 0.6962746537368745\n",
      "epoch 0 loss at step: 25000 is 0.4473756550785154\n",
      "epoch 0 loss at step: 30000 is 0.30860093593299387\n",
      "epoch 0 loss at step: 35000 is 0.2442272994570434\n",
      "epoch 0 loss at step: 40000 is 0.1912742623878643\n",
      "epoch 0 loss at step: 45000 is 0.18337213502191008\n",
      "epoch 0 loss at step: 50000 is 0.1768284473521635\n",
      "epoch 0 loss at step: 55000 is 0.16804007335193455\n",
      "epoch 0 loss at step: 60000 is 0.16960205589663238\n",
      "epoch 0 loss at step: 65000 is 0.16165804571742193\n",
      "epoch 0 loss at step: 70000 is 0.16408938187602906\n",
      "epoch 0 loss at step: 75000 is 0.1562148043267429\n",
      "epoch 0 loss at step: 80000 is 0.15470766914086417\n",
      "epoch 0 loss at step: 85000 is 0.16074717778009362\n",
      "epoch 0 loss at step: 90000 is 0.16170650744186715\n",
      "epoch 0 loss at step: 95000 is 0.16077488029636441\n",
      "epoch 0 loss at step: 100000 is 0.16046097958660685\n",
      "epoch 0 loss at step: 105000 is 0.16165988103449344\n",
      "epoch 0 loss at step: 110000 is 0.16051051253713666\n",
      "epoch 0 loss at step: 115000 is 0.15684879509098829\n",
      "epoch 0 loss at step: 120000 is 0.16306768654547632\n",
      "epoch 0 loss at step: 125000 is 0.16277943282108753\n",
      "epoch 0 loss at step: 130000 is 0.15500537731721997\n",
      "epoch 0 loss at step: 135000 is 0.15639037596452982\n",
      "epoch 0 loss at step: 140000 is 0.15955802283883094\n",
      "epoch 0 loss at step: 145000 is 0.15523079238794743\n",
      "epoch 0 loss at step: 150000 is 0.16018991997474805\n",
      "epoch 0 loss at step: 155000 is 0.15229803361929953\n",
      "epoch 0 loss at step: 160000 is 0.15516295402674005\n",
      "epoch 0 loss at step: 165000 is 0.1548144543370232\n",
      "epoch 0 loss at step: 170000 is 0.14872504432671704\n",
      "epoch 0 loss at step: 175000 is 0.14525145912822335\n",
      "epoch 0 loss at step: 180000 is 0.15530869387513957\n",
      "epoch 0 loss at step: 185000 is 0.1527479712633416\n",
      "epoch 0 loss at step: 190000 is 0.15192622973285616\n",
      "epoch 0 loss at step: 195000 is 0.1578003090839833\n",
      "epoch 0 loss at step: 200000 is 0.14922742161322386\n",
      "epoch 0 loss at step: 205000 is 0.15472127782804893\n",
      "epoch 0 loss at step: 210000 is 0.15229881066717207\n",
      "epoch 0 loss at step: 215000 is 0.15332469871044158\n",
      "epoch 0 loss at step: 220000 is 0.15448160826656968\n",
      "epoch 0 loss at step: 225000 is 0.14247036810033023\n",
      "epoch 0 loss at step: 230000 is 0.15819807633273303\n",
      "epoch 0 loss at step: 235000 is 0.15603556024399587\n",
      "epoch 0 loss at step: 240000 is 0.15590344836004078\n",
      "epoch 0 loss at step: 245000 is 0.15218341842573135\n",
      "epoch 0 loss at step: 250000 is 0.15997623335160316\n",
      "epoch 0 loss at step: 255000 is 0.14685146398413926\n",
      "epoch 0 loss at step: 260000 is 0.1444239749509841\n",
      "epoch 0 loss at step: 265000 is 0.1543488332239911\n",
      "epoch 0 loss at step: 270000 is 0.153249303450156\n",
      "epoch 0 loss at step: 275000 is 0.1483869198087603\n",
      "epoch 0 loss at step: 280000 is 0.1482890972768888\n",
      "epoch 0 loss at step: 285000 is 0.15092672252599149\n",
      "epoch 0 loss at step: 290000 is 0.14406909995302558\n",
      "epoch 0 loss at step: 295000 is 0.15611984533667564\n",
      "epoch 0 loss at step: 300000 is 0.15499192416276783\n",
      "epoch 0 loss at step: 305000 is 0.14902664945218713\n",
      "epoch 0 loss at step: 310000 is 0.1564828314648941\n",
      "epoch 0 loss at step: 315000 is 0.14668273408222013\n",
      "epoch 0 loss at step: 320000 is 0.1492744044130668\n",
      "epoch 0 loss at step: 325000 is 0.1504121398093179\n",
      "epoch 0 loss at step: 330000 is 0.15148478531464935\n",
      "epoch 0 loss at step: 335000 is 0.1582109222408384\n",
      "epoch 0 loss at step: 340000 is 0.1515591696439311\n",
      "epoch 0 loss at step: 345000 is 0.15008259894195944\n",
      "epoch 0 loss at step: 350000 is 0.15050451602591203\n",
      "epoch 0 loss at step: 355000 is 0.15188626921772957\n",
      "epoch 0 loss at step: 360000 is 0.15238103133710101\n",
      "epoch 0 loss at step: 365000 is 0.15181093848524616\n",
      "epoch 0 loss at step: 370000 is 0.1525877573441714\n",
      "epoch 0 loss at step: 375000 is 0.15084174285940827\n",
      "epoch 0 loss at step: 380000 is 0.14630312021598219\n",
      "epoch 0 loss at step: 385000 is 0.14341608903948216\n",
      "epoch 0 loss at step: 390000 is 0.15505415567634628\n",
      "epoch 0 loss at step: 395000 is 0.14764403783529997\n",
      "epoch 0 loss at step: 400000 is 0.14563271686285734\n",
      "epoch 0 loss at step: 405000 is 0.1481529918340035\n",
      "epoch 0 loss at step: 410000 is 0.15465088672675192\n",
      "epoch 0 loss at step: 415000 is 0.15330127018541098\n",
      "epoch 0 loss at step: 420000 is 0.1510827189495787\n",
      "epoch 0 loss at step: 425000 is 0.14616051721498372\n",
      "epoch 0 loss at step: 430000 is 0.14851957136765123\n",
      "epoch 0 loss at step: 435000 is 0.1502707841041498\n",
      "epoch 0 loss at step: 440000 is 0.14326127048712223\n",
      "epoch 0 loss at step: 445000 is 0.1521893566192128\n",
      "epoch 0 loss at step: 450000 is 0.1426421760696918\n",
      "epoch 0 loss at step: 455000 is 0.15337251189332454\n",
      "epoch 0 loss at step: 460000 is 0.152592905494757\n",
      "epoch 0 loss at step: 465000 is 0.14619650881513954\n",
      "epoch 0 loss at step: 470000 is 0.14870731643391774\n",
      "epoch 0 loss at step: 475000 is 0.14785204237736763\n",
      "epoch 0 loss at step: 480000 is 0.15237997167836875\n",
      "epoch 0 loss at step: 485000 is 0.1468993656530045\n",
      "epoch 0 loss at step: 490000 is 0.15609812914226204\n",
      "epoch 0 loss at step: 495000 is 0.1521356002356857\n",
      "epoch 0 loss at step: 500000 is 0.14684830724988132\n",
      "epoch 0 loss at step: 505000 is 0.14027449748553336\n",
      "epoch 0 loss at step: 510000 is 0.14937778388848527\n",
      "epoch 0 loss at step: 515000 is 0.14568480401253328\n",
      "epoch 0 loss at step: 520000 is 0.14632802395066247\n",
      "epoch 0 loss at step: 525000 is 0.14493796998346226\n",
      "epoch 0 loss at step: 530000 is 0.14490924997244148\n",
      "epoch 0 loss at step: 535000 is 0.15286447865366937\n",
      "epoch 0 loss at step: 540000 is 0.15409774980340152\n",
      "epoch 0 loss at step: 545000 is 0.14660015924163164\n",
      "epoch 0 loss at step: 550000 is 0.15033151757614688\n",
      "epoch 0 loss at step: 555000 is 0.14674405051060022\n",
      "epoch 0 loss at step: 560000 is 0.1449324082569685\n",
      "epoch 0 loss at step: 565000 is 0.14080892756748944\n",
      "epoch 0 loss at step: 570000 is 0.14241999438051134\n",
      "epoch 0 loss at step: 575000 is 0.14858803606126458\n",
      "epoch 0 loss at step: 580000 is 0.14319827895034104\n",
      "epoch 0 loss at step: 585000 is 0.14254090013122186\n",
      "epoch 0 loss at step: 590000 is 0.14711081452593208\n",
      "epoch 0 loss at step: 595000 is 0.1433060996416956\n",
      "epoch 0 loss at step: 600000 is 0.14252200629375875\n",
      "epoch 0 loss at step: 605000 is 0.14008402429847047\n",
      "epoch 0 loss at step: 610000 is 0.15214391069570557\n",
      "epoch 0 loss at step: 615000 is 0.14804259330816566\n",
      "epoch 0 loss at step: 620000 is 0.14713778931237756\n",
      "epoch 0 loss at step: 625000 is 0.1493522605182603\n",
      "epoch 0 loss at step: 630000 is 0.1540872221933678\n",
      "epoch 0 loss at step: 635000 is 0.14201506710182876\n",
      "epoch 0 loss at step: 640000 is 0.15054843161404133\n",
      "epoch 0 loss at step: 645000 is 0.14228670698851348\n",
      "epoch 0 loss at step: 650000 is 0.14692247566264122\n",
      "epoch 0 loss at step: 655000 is 0.14814422477893532\n",
      "epoch 0 loss at step: 660000 is 0.14621444752961396\n",
      "epoch 0 loss at step: 665000 is 0.14582314659086987\n",
      "epoch 0 loss at step: 670000 is 0.14872248720154166\n",
      "epoch 0 loss at step: 675000 is 0.1441149961596355\n",
      "epoch 0 loss at step: 680000 is 0.15171362522877752\n",
      "epoch 0 loss at step: 685000 is 0.14792233591564\n",
      "epoch 0 loss at step: 690000 is 0.15210799060752617\n",
      "epoch 0 loss at step: 695000 is 0.149646687403135\n",
      "epoch 0 loss at step: 700000 is 0.14012676574743818\n",
      "epoch 0 loss at step: 705000 is 0.14291563526839018\n",
      "epoch 0 loss at step: 710000 is 0.14144667807891964\n",
      "epoch 0 loss at step: 715000 is 0.14679932013022481\n",
      "epoch 0 loss at step: 720000 is 0.14795744244232772\n",
      "epoch 0 loss at step: 725000 is 0.14810603950098156\n",
      "epoch 0 loss at step: 730000 is 0.14226190352430568\n",
      "epoch 0 loss at step: 735000 is 0.1394512759592384\n",
      "epoch 0 loss at step: 740000 is 0.1471286922847852\n",
      "epoch 0 loss at step: 745000 is 0.1422249194137752\n",
      "epoch 0 loss at step: 750000 is 0.13989926702771335\n",
      "epoch 0 loss at step: 755000 is 0.1366306656697765\n",
      "epoch 0 loss at step: 760000 is 0.1421643459999934\n",
      "epoch 0 loss at step: 765000 is 0.14792845818605274\n",
      "epoch 0 loss at step: 770000 is 0.14244933069851248\n",
      "epoch 0 loss at step: 775000 is 0.1371409804484807\n",
      "epoch 0 loss at step: 780000 is 0.1435039616432041\n",
      "epoch 0 loss at step: 785000 is 0.14357880751017946\n",
      "epoch 0 loss at step: 790000 is 0.15065959161184728\n",
      "epoch 0 loss at step: 795000 is 0.14473411232698707\n",
      "epoch 0 loss at step: 800000 is 0.1391272162218578\n",
      "epoch 0 loss at step: 805000 is 0.13870504447747953\n",
      "epoch 0 loss at step: 810000 is 0.14511069647641853\n",
      "epoch 0 loss at step: 815000 is 0.14490472626071424\n",
      "epoch 0 loss at step: 820000 is 0.14183836012855172\n",
      "epoch 0 loss at step: 825000 is 0.1431373505838681\n",
      "epoch 0 loss at step: 830000 is 0.1451400907471776\n",
      "epoch 0 loss at step: 835000 is 0.1375605609189719\n",
      "epoch 0 loss at step: 840000 is 0.13423028808310628\n",
      "epoch 0 loss at step: 845000 is 0.14079368838146328\n",
      "epoch 0 loss at step: 850000 is 0.14221409149430692\n",
      "epoch 0 loss at step: 855000 is 0.14883726302478462\n",
      "epoch 0 loss at step: 860000 is 0.1453608706000261\n",
      "epoch 0 loss at step: 865000 is 0.144992365148291\n",
      "epoch 0 loss at step: 870000 is 0.14791883447578874\n",
      "epoch 0 loss at step: 875000 is 0.142494792587962\n",
      "epoch 0 loss at step: 880000 is 0.1464711818130687\n",
      "epoch 0 loss at step: 885000 is 0.13736989738876\n",
      "epoch 0 loss at step: 890000 is 0.13990349142439665\n",
      "epoch 0 loss at step: 895000 is 0.1467140148565173\n",
      "epoch 0 loss at step: 900000 is 0.14896865907451137\n",
      "epoch 0 loss at step: 905000 is 0.14003345491737126\n",
      "epoch 0 loss at step: 910000 is 0.14124262121208012\n",
      "epoch 0 loss at step: 915000 is 0.14041976667009293\n",
      "epoch 0 loss at step: 920000 is 0.13551657651569693\n",
      "epoch 0 loss at step: 925000 is 0.14620180307030678\n",
      "epoch 0 loss at step: 930000 is 0.14358333731740713\n",
      "epoch 0 loss at step: 935000 is 0.14171230055522174\n",
      "epoch 0 loss at step: 940000 is 0.1432410930717364\n",
      "epoch 0 loss at step: 945000 is 0.1396962572529912\n",
      "epoch 0 loss at step: 950000 is 0.14598674845509232\n",
      "epoch 0 loss at step: 955000 is 0.14667988727930933\n",
      "epoch 0 loss at step: 960000 is 0.13333579362817108\n",
      "epoch 0 loss at step: 965000 is 0.14672861089888029\n",
      "epoch 0 loss at step: 970000 is 0.13805030438899993\n",
      "epoch 0 loss at step: 975000 is 0.1431414154717\n",
      "epoch 0 loss at step: 980000 is 0.13773495452702045\n",
      "epoch 0 loss at step: 985000 is 0.14249419223051518\n",
      "epoch 0 loss at step: 990000 is 0.13703486025473102\n",
      "epoch 0 loss at step: 995000 is 0.14474696841426193\n",
      "epoch 0 loss at step: 1000000 is 0.1361419940199703\n",
      "epoch 0 loss at step: 1005000 is 0.1403995450032875\n",
      "epoch 0 loss at step: 1010000 is 0.14066610436532648\n",
      "epoch 0 loss at step: 1015000 is 0.13800518468804657\n",
      "epoch 0 loss at step: 1020000 is 0.14088801712952553\n",
      "epoch 0 loss at step: 1025000 is 0.13677634604526684\n",
      "epoch 0 loss at step: 1030000 is 0.13316012877579778\n",
      "epoch 0 loss at step: 1035000 is 0.13959046902274713\n",
      "epoch 0 loss at step: 1040000 is 0.1387553461847827\n",
      "epoch 0 loss at step: 1045000 is 0.1310446930395439\n",
      "epoch 0 loss at step: 1050000 is 0.1387742602897808\n",
      "epoch 0 loss at step: 1055000 is 0.1329198084000498\n",
      "epoch 0 loss at step: 1060000 is 0.1378894393437542\n",
      "epoch 0 loss at step: 1065000 is 0.13920811175890266\n",
      "epoch 0 loss at step: 1070000 is 0.14449434238933026\n",
      "epoch 0 loss at step: 1075000 is 0.13874654944809153\n",
      "epoch 0 loss at step: 1080000 is 0.1396161028387025\n",
      "epoch 0 loss at step: 1085000 is 0.1408398855995387\n",
      "epoch 0 loss at step: 1090000 is 0.13513635091935283\n",
      "epoch 0 loss at step: 1095000 is 0.1377855373904109\n",
      "epoch 0 loss at step: 1100000 is 0.1428426117135212\n",
      "epoch 0 loss at step: 1105000 is 0.1391370324753225\n",
      "epoch 0 loss at step: 1110000 is 0.13938703364380636\n",
      "epoch 0 loss at step: 1115000 is 0.14064581994246692\n",
      "epoch 0 loss at step: 1120000 is 0.13959163368344307\n",
      "epoch 0 loss at step: 1125000 is 0.13908542702775448\n",
      "epoch 0 loss at step: 1130000 is 0.14228621831741184\n",
      "epoch 0 loss at step: 1135000 is 0.1381493235663511\n",
      "epoch 0 loss at step: 1140000 is 0.13991867886558176\n",
      "epoch 0 loss at step: 1145000 is 0.13692617573672905\n",
      "epoch 0 loss at step: 1150000 is 0.135923740965873\n",
      "epoch 0 loss at step: 1155000 is 0.13715790536291897\n",
      "epoch 0 loss at step: 1160000 is 0.13763384540881962\n",
      "epoch 0 loss at step: 1165000 is 0.1404820559025742\n",
      "epoch 0 loss at step: 1170000 is 0.13865138006694616\n",
      "epoch 0 loss at step: 1175000 is 0.14118812989082652\n",
      "epoch 0 loss at step: 1180000 is 0.14380536035877886\n",
      "epoch 0 loss at step: 1185000 is 0.13754497892996295\n",
      "epoch 0 loss at step: 1190000 is 0.13646010419670493\n",
      "epoch 0 loss at step: 1195000 is 0.13771646313741803\n",
      "epoch 0 loss at step: 1200000 is 0.1358012248378247\n",
      "epoch 0 loss at step: 1205000 is 0.13832351598925888\n",
      "epoch 0 loss at step: 1210000 is 0.13821316881943493\n",
      "epoch 0 loss at step: 1215000 is 0.13603949237344787\n",
      "epoch 0 loss at step: 1220000 is 0.13864396758950315\n",
      "epoch 0 loss at step: 1225000 is 0.14327704000212252\n",
      "epoch 0 loss at step: 1230000 is 0.13601116239735855\n",
      "epoch 0 loss at step: 1235000 is 0.13690607130513527\n",
      "epoch 0 loss at step: 1240000 is 0.14006685027852656\n",
      "epoch 0 loss at step: 1245000 is 0.1342613447090611\n",
      "epoch 0 loss at step: 1250000 is 0.13620201829280704\n",
      "epoch 0 loss at step: 1255000 is 0.13471466082371772\n",
      "epoch 0 loss at step: 1260000 is 0.1371142698891461\n",
      "epoch 0 loss at step: 1265000 is 0.13502627852619625\n",
      "epoch 0 loss at step: 1270000 is 0.13770101385191083\n",
      "epoch 0 loss at step: 1275000 is 0.13656790651874617\n",
      "epoch 0 loss at step: 1280000 is 0.13760927969049663\n",
      "epoch 0 loss at step: 1285000 is 0.1278941176975146\n",
      "epoch 0 loss at step: 1290000 is 0.13512999014491214\n",
      "epoch 0 loss at step: 1295000 is 0.1383389110205695\n",
      "epoch 0 loss at step: 1300000 is 0.13322504277713595\n",
      "epoch 0 loss at step: 1305000 is 0.13390755547471345\n",
      "epoch 0 loss at step: 1310000 is 0.13208783977981656\n",
      "epoch 0 loss at step: 1315000 is 0.13711205744426697\n",
      "epoch 0 loss at step: 1320000 is 0.13662181318956426\n",
      "epoch 0 loss at step: 1325000 is 0.13846438899515195\n",
      "epoch 0 loss at step: 1330000 is 0.13821151451752522\n",
      "epoch 0 loss at step: 1335000 is 0.13064887491948904\n",
      "epoch 0 loss at step: 1340000 is 0.13486784844100475\n",
      "epoch 0 loss at step: 1345000 is 0.13795537290479987\n",
      "epoch 0 loss at step: 1350000 is 0.12908336615897714\n",
      "epoch 0 loss at step: 1355000 is 0.1341192221911624\n",
      "epoch 0 loss at step: 1360000 is 0.13148654158916323\n",
      "epoch 0 loss at step: 1365000 is 0.1292398877710104\n",
      "epoch 0 loss at step: 1370000 is 0.13166686290558427\n",
      "epoch 0 loss at step: 1375000 is 0.1293543188353069\n",
      "epoch 0 loss at step: 1380000 is 0.14121012377366424\n",
      "epoch 0 loss at step: 1385000 is 0.13660174132827668\n",
      "epoch 0 loss at step: 1390000 is 0.1304820406721905\n",
      "epoch 0 loss at step: 1395000 is 0.13627573538832366\n",
      "epoch 0 loss at step: 1400000 is 0.1340327158059925\n",
      "epoch 0 loss at step: 1405000 is 0.1373016442362219\n",
      "epoch 0 loss at step: 1410000 is 0.13701276457514613\n",
      "epoch 0 loss at step: 1415000 is 0.13847643125578762\n",
      "epoch 0 loss at step: 1420000 is 0.13400493391323834\n",
      "epoch 0 loss at step: 1425000 is 0.1338046157585457\n",
      "epoch 0 loss at step: 1430000 is 0.13794113223822788\n",
      "epoch 0 loss at step: 1435000 is 0.13453061781618744\n",
      "epoch 0 loss at step: 1440000 is 0.12778464526403696\n",
      "epoch 0 loss at step: 1445000 is 0.13382790502421557\n",
      "epoch 0 loss at step: 1450000 is 0.13348841149955987\n",
      "epoch 0 loss at step: 1455000 is 0.12706058872379364\n",
      "epoch 0 loss at step: 1460000 is 0.13233887189868837\n",
      "epoch 0 loss at step: 1465000 is 0.13046384974326938\n",
      "epoch 0 loss at step: 1470000 is 0.13341919107288122\n",
      "epoch 0 loss at step: 1475000 is 0.1356571511618793\n",
      "epoch 0 loss at step: 1480000 is 0.13252607262972743\n",
      "epoch 0 loss at step: 1485000 is 0.12912392973091918\n",
      "epoch 0 loss at step: 1490000 is 0.13357238120054826\n",
      "epoch 0 loss at step: 1495000 is 0.1329940162871033\n",
      "epoch 0 loss at step: 1500000 is 0.13782166282553227\n",
      "epoch 0 loss at step: 1505000 is 0.1333002302695997\n",
      "epoch 0 loss at step: 1510000 is 0.13056376008801163\n",
      "epoch 0 loss at step: 1515000 is 0.13254628795664758\n",
      "epoch 0 loss at step: 1520000 is 0.1280244894847274\n",
      "epoch 0 loss at step: 1525000 is 0.1278517978478223\n",
      "epoch 0 loss at step: 1530000 is 0.12951662704925984\n",
      "epoch 0 loss at step: 1535000 is 0.1293325577323325\n",
      "epoch 0 loss at step: 1540000 is 0.13064450479662046\n",
      "epoch 0 loss at step: 1545000 is 0.13093445419799538\n",
      "epoch 0 loss at step: 1550000 is 0.12960540002901108\n",
      "epoch 0 loss at step: 1555000 is 0.1306427544379607\n",
      "epoch 0 loss at step: 1560000 is 0.13291128400918095\n",
      "epoch 0 loss at step: 1565000 is 0.12991154392920434\n",
      "epoch 0 loss at step: 1570000 is 0.13123450291557237\n",
      "epoch 0 loss at step: 1575000 is 0.12438383211833424\n",
      "epoch 0 loss at step: 1580000 is 0.13536103061782195\n",
      "epoch 0 loss at step: 1585000 is 0.13398649454070255\n",
      "epoch 0 loss at step: 1590000 is 0.1315286126030609\n",
      "epoch 0 loss at step: 1595000 is 0.13291173594817518\n",
      "epoch 0 loss at step: 1600000 is 0.1322688730851747\n",
      "epoch 0 loss at step: 1605000 is 0.1311414599268697\n",
      "epoch 0 loss at step: 1610000 is 0.12942416424620898\n",
      "epoch 0 loss at step: 1615000 is 0.1341099127853289\n",
      "epoch 0 loss at step: 1620000 is 0.13352506237654016\n",
      "epoch 0 loss at step: 1625000 is 0.1329051456393674\n",
      "epoch 0 loss at step: 1630000 is 0.13345349516561256\n",
      "epoch 0 loss at step: 1635000 is 0.13195714137041942\n",
      "epoch 0 loss at step: 1640000 is 0.13056351108266973\n",
      "epoch 0 loss at step: 1645000 is 0.13512071931324898\n",
      "epoch 0 loss at step: 1650000 is 0.13263645646739752\n",
      "epoch 0 loss at step: 1655000 is 0.1298676544142887\n",
      "epoch 0 loss at step: 1660000 is 0.13366810776405036\n",
      "epoch 0 loss at step: 1665000 is 0.12666560523384252\n",
      "epoch 0 loss at step: 1670000 is 0.12980998704060911\n",
      "epoch 0 loss at step: 1675000 is 0.1336409117246978\n",
      "epoch 0 loss at step: 1680000 is 0.12931519274068995\n",
      "epoch 0 loss at step: 1685000 is 0.13010995985548943\n",
      "epoch 0 loss at step: 1690000 is 0.1265501926358789\n",
      "epoch 0 loss at step: 1695000 is 0.13046730363741518\n",
      "epoch 0 loss at step: 1700000 is 0.13302700340542942\n",
      "epoch 0 loss at step: 1705000 is 0.13134069848749785\n",
      "epoch 0 loss at step: 1710000 is 0.130764317587018\n",
      "epoch 0 loss at step: 1715000 is 0.1262100218183361\n",
      "epoch 0 loss at step: 1720000 is 0.13067152337506413\n",
      "epoch 0 loss at step: 1725000 is 0.13187054963037373\n",
      "epoch 0 loss at step: 1730000 is 0.1281299721811898\n",
      "epoch 0 loss at step: 1735000 is 0.1345032540563494\n",
      "epoch 0 loss at step: 1740000 is 0.13153355808891357\n",
      "epoch 0 loss at step: 1745000 is 0.12774467288088054\n",
      "epoch 0 loss at step: 1750000 is 0.1323859492796473\n",
      "epoch 0 loss at step: 1755000 is 0.13273715913351625\n",
      "epoch 0 loss at step: 1760000 is 0.13416629527341575\n",
      "epoch 0 loss at step: 1765000 is 0.12690256534302607\n",
      "epoch 0 loss at step: 1770000 is 0.12775872386181727\n",
      "epoch 0 loss at step: 1775000 is 0.1348135992575437\n",
      "epoch 0 loss at step: 1780000 is 0.12562518568895759\n",
      "epoch 0 loss at step: 1785000 is 0.12695707168970258\n",
      "epoch 0 loss at step: 1790000 is 0.1284996345449239\n",
      "epoch 0 loss at step: 1795000 is 0.12899125739764422\n",
      "epoch 0 loss at step: 1800000 is 0.13361839846726506\n",
      "epoch 0 loss at step: 1805000 is 0.13047002688506618\n",
      "epoch 0 loss at step: 1810000 is 0.13177634632140398\n",
      "epoch 0 loss at step: 1815000 is 0.12793402676284313\n",
      "epoch 0 loss at step: 1820000 is 0.12301629062332213\n",
      "epoch 0 loss at step: 1825000 is 0.1249054310053587\n",
      "epoch 0 loss at step: 1830000 is 0.13056086212182416\n",
      "epoch 0 loss at step: 1835000 is 0.12897626477684826\n",
      "epoch 0 loss at step: 1840000 is 0.13835821907296778\n",
      "epoch 0 loss at step: 1845000 is 0.13048227252606304\n",
      "epoch 0 loss at step: 1850000 is 0.125050018560607\n",
      "epoch 0 loss at step: 1855000 is 0.126988804577291\n",
      "epoch 0 loss at step: 1860000 is 0.12311977038290352\n",
      "epoch 0 loss at step: 1865000 is 0.13290063350759446\n",
      "epoch 0 loss at step: 1870000 is 0.13152813862953336\n",
      "epoch 0 loss at step: 1875000 is 0.13834721557777374\n",
      "epoch 0 loss at step: 1880000 is 0.13168155119419098\n",
      "epoch 0 loss at step: 1885000 is 0.12437860577460379\n",
      "epoch 0 loss at step: 1890000 is 0.12791218462195247\n",
      "epoch 0 loss at step: 1895000 is 0.12431912362650037\n",
      "epoch 0 loss at step: 1900000 is 0.12477506298897788\n",
      "epoch 0 loss at step: 1905000 is 0.12221667545642703\n",
      "epoch 0 loss at step: 1910000 is 0.12912860667100176\n",
      "epoch 0 loss at step: 1915000 is 0.12651881278082727\n",
      "epoch 0 loss at step: 1920000 is 0.12770136342607438\n",
      "epoch 0 loss at step: 1925000 is 0.12757487548030913\n",
      "epoch 0 loss at step: 1930000 is 0.12959854580936953\n",
      "epoch 0 loss at step: 1935000 is 0.12182026535244658\n",
      "epoch 0 loss at step: 1940000 is 0.13213518654759973\n",
      "epoch 0 loss at step: 1945000 is 0.1283493939523585\n",
      "epoch 0 loss at step: 1950000 is 0.12674596269493923\n",
      "epoch 0 loss at step: 1955000 is 0.12641936321328395\n",
      "epoch 0 loss at step: 1960000 is 0.12662741823866963\n",
      "epoch 0 loss at step: 1965000 is 0.12981023719040677\n",
      "epoch 0 loss at step: 1970000 is 0.1267784001659602\n",
      "epoch 0 loss at step: 1975000 is 0.12892522360077127\n",
      "epoch 0 loss at step: 1980000 is 0.12629773063007743\n",
      "epoch 0 loss at step: 1985000 is 0.12491776377819479\n",
      "epoch 0 loss at step: 1990000 is 0.12565167196504773\n",
      "epoch 0 loss at step: 1995000 is 0.12736997181922197\n",
      "epoch 0 loss at step: 2000000 is 0.13624325832305476\n",
      "epoch 0 loss at step: 2005000 is 0.12592506405711174\n",
      "epoch 0 loss at step: 2010000 is 0.12643557471446693\n",
      "epoch 0 loss at step: 2015000 is 0.12661748044583945\n",
      "epoch 0 loss at step: 2020000 is 0.1284968368180096\n",
      "epoch 0 loss at step: 2025000 is 0.13024417878845707\n",
      "epoch 0 loss at step: 2030000 is 0.12581827570945026\n",
      "epoch 0 loss at step: 2035000 is 0.12647254133597016\n",
      "epoch 0 loss at step: 2040000 is 0.12653457701783627\n",
      "epoch 0 loss at step: 2045000 is 0.12791484138276427\n",
      "epoch 0 loss at step: 2050000 is 0.12883632486760616\n",
      "epoch 0 loss at step: 2055000 is 0.1300875364203006\n",
      "epoch 0 loss at step: 2060000 is 0.12462191740591079\n",
      "epoch 0 loss at step: 2065000 is 0.12512127210423352\n",
      "epoch 0 loss at step: 2070000 is 0.1281332489049062\n",
      "epoch 0 loss at step: 2075000 is 0.12065409203786404\n",
      "epoch 0 loss at step: 2080000 is 0.12417709467168897\n",
      "epoch 0 loss at step: 2085000 is 0.11811655063270592\n",
      "epoch 0 loss at step: 2090000 is 0.11824640797618777\n",
      "epoch 0 loss at step: 2095000 is 0.1303357980363071\n",
      "epoch 0 loss at step: 2100000 is 0.12314004084626212\n",
      "epoch 0 loss at step: 2105000 is 0.1226091901326552\n",
      "epoch 0 loss at step: 2110000 is 0.12293596567045897\n",
      "epoch 0 loss at step: 2115000 is 0.13048254576958715\n",
      "epoch 0 loss at step: 2120000 is 0.12702928325328977\n",
      "epoch 0 loss at step: 2125000 is 0.12307898647859693\n",
      "epoch 0 loss at step: 2130000 is 0.12465089117772878\n",
      "epoch 0 loss at step: 2135000 is 0.12602467753328384\n",
      "epoch 0 loss at step: 2140000 is 0.1256958252045326\n",
      "epoch 0 loss at step: 2145000 is 0.12295465786084532\n",
      "epoch 0 loss at step: 2150000 is 0.12038385625323281\n",
      "epoch 0 loss at step: 2155000 is 0.12041120422091335\n",
      "epoch 0 loss at step: 2160000 is 0.12847010402213782\n",
      "epoch 0 loss at step: 2165000 is 0.13073148189745842\n",
      "epoch 0 loss at step: 2170000 is 0.1278497855000198\n",
      "epoch 0 loss at step: 2175000 is 0.1251943184385542\n",
      "epoch 0 loss at step: 2180000 is 0.12608942146580665\n",
      "epoch 0 loss at step: 2185000 is 0.1251364573849365\n",
      "epoch 0 loss at step: 2190000 is 0.12847730866568163\n",
      "epoch 0 loss at step: 2195000 is 0.12519136376455425\n",
      "epoch 0 loss at step: 2200000 is 0.12811061553750186\n",
      "epoch 0 loss at step: 2205000 is 0.1266705579251051\n",
      "epoch 0 loss at step: 2210000 is 0.12574495395123958\n",
      "epoch 0 loss at step: 2215000 is 0.12639305740734563\n",
      "epoch 0 loss at step: 2220000 is 0.1228823763590306\n",
      "epoch 0 loss at step: 2225000 is 0.12280296249184758\n",
      "epoch 0 loss at step: 2230000 is 0.1174758522670716\n",
      "epoch 0 loss at step: 2235000 is 0.1292168543389067\n",
      "epoch 0 loss at step: 2240000 is 0.12336708603519947\n",
      "epoch 0 loss at step: 2245000 is 0.12231139260251075\n",
      "epoch 0 loss at step: 2250000 is 0.12045735054896213\n",
      "epoch 0 loss at step: 2255000 is 0.12327341695791111\n",
      "epoch 0 loss at step: 2260000 is 0.121210265696235\n",
      "epoch 0 loss at step: 2265000 is 0.12316892260368914\n",
      "epoch 0 loss at step: 2270000 is 0.12674961275430396\n",
      "epoch 0 loss at step: 2275000 is 0.12621027149758302\n",
      "epoch 0 loss at step: 2280000 is 0.11687122562713921\n",
      "epoch 0 loss at step: 2285000 is 0.12161688123494387\n",
      "epoch 0 loss at step: 2290000 is 0.12181835842896253\n",
      "epoch 0 loss at step: 2295000 is 0.12766829575023148\n",
      "epoch 0 loss at step: 2300000 is 0.12606025899443776\n",
      "epoch 0 loss at step: 2305000 is 0.12486043112538756\n",
      "epoch 0 loss at step: 2310000 is 0.1321625536158681\n",
      "epoch 0 loss at step: 2315000 is 0.12638571259370074\n",
      "epoch 0 loss at step: 2320000 is 0.12057779408907518\n",
      "epoch 0 loss at step: 2325000 is 0.11954064438007772\n",
      "epoch 0 loss at step: 2330000 is 0.1228249581316486\n",
      "epoch 0 loss at step: 2335000 is 0.12321770909396\n",
      "epoch 0 loss at step: 2340000 is 0.1211176708785817\n",
      "epoch 0 loss at step: 2345000 is 0.1257777673304081\n",
      "epoch 0 loss at step: 2350000 is 0.12023291383804753\n",
      "epoch 0 loss at step: 2355000 is 0.12361084635853767\n",
      "epoch 0 loss at step: 2360000 is 0.1253152836667374\n",
      "epoch 0 loss at step: 2365000 is 0.11970508868694306\n",
      "epoch 0 loss at step: 2370000 is 0.11993234346834943\n",
      "epoch 0 loss at step: 2375000 is 0.12688292386876418\n",
      "epoch 0 loss at step: 2380000 is 0.12168398293945938\n",
      "epoch 0 loss at step: 2385000 is 0.1282806321677752\n",
      "epoch 0 loss at step: 2390000 is 0.12255501920953393\n",
      "epoch 0 loss at step: 2395000 is 0.1256422059347853\n",
      "epoch 0 loss at step: 2400000 is 0.12496972723593935\n",
      "epoch 0 loss at step: 2405000 is 0.1234926232483238\n",
      "epoch 0 loss at step: 2410000 is 0.12764069514553994\n",
      "epoch 0 loss at step: 2415000 is 0.12351036865096539\n",
      "epoch 0 loss at step: 2420000 is 0.1164243918848224\n",
      "epoch 0 loss at step: 2425000 is 0.1275030627185479\n",
      "epoch 0 loss at step: 2430000 is 0.11755302153890952\n",
      "epoch 0 loss at step: 2435000 is 0.12610993223469705\n",
      "epoch 0 loss at step: 2440000 is 0.12530818008035421\n",
      "epoch 0 loss at step: 2445000 is 0.12707871687058359\n",
      "epoch 0 loss at step: 2450000 is 0.12406854032194242\n",
      "epoch 0 loss at step: 2455000 is 0.12356049616690726\n",
      "epoch 0 loss at step: 2460000 is 0.12542837727628647\n",
      "epoch 0 loss at step: 2465000 is 0.12505231001637876\n",
      "epoch 0 loss at step: 2470000 is 0.12540708877746948\n",
      "epoch 0 loss at step: 2475000 is 0.12085846167393029\n",
      "epoch 0 loss at step: 2480000 is 0.12388769583594986\n",
      "epoch 0 loss at step: 2485000 is 0.1273088756569661\n",
      "epoch 0 loss at step: 2490000 is 0.11954527696305886\n",
      "epoch 0 loss at step: 2495000 is 0.1211220846299082\n",
      "epoch 0 loss at step: 2500000 is 0.12108549737390131\n",
      "epoch 0 loss at step: 2505000 is 0.1210503185740672\n",
      "epoch 0 loss at step: 2510000 is 0.11664278517337516\n",
      "epoch 0 loss at step: 2515000 is 0.11874948759218677\n",
      "epoch 0 loss at step: 2520000 is 0.11777072913423181\n",
      "epoch 0 loss at step: 2525000 is 0.1205162614930421\n",
      "epoch 0 loss at step: 2530000 is 0.12355985655989497\n",
      "epoch 0 loss at step: 2535000 is 0.1239401206647046\n",
      "epoch 0 loss at step: 2540000 is 0.11955171565809287\n",
      "epoch 0 loss at step: 2545000 is 0.12178731614351272\n",
      "epoch 0 loss at step: 2550000 is 0.12815288684368134\n",
      "epoch 0 loss at step: 2555000 is 0.12288119800500572\n",
      "epoch 0 loss at step: 2560000 is 0.12546755947428756\n",
      "epoch 0 loss at step: 2565000 is 0.12119306128509343\n",
      "epoch 0 loss at step: 2570000 is 0.11651918607533444\n",
      "epoch 0 loss at step: 2575000 is 0.12429677953161299\n",
      "epoch 0 loss at step: 2580000 is 0.1219311901146546\n",
      "epoch 0 loss at step: 2585000 is 0.1173592346990481\n",
      "epoch 0 loss at step: 2590000 is 0.12087568057337776\n",
      "epoch 0 loss at step: 2595000 is 0.12219629642441869\n",
      "epoch 0 loss at step: 2600000 is 0.12609942824169992\n",
      "epoch 0 loss at step: 2605000 is 0.11979784301109613\n",
      "epoch 0 loss at step: 2610000 is 0.11921986826714129\n",
      "epoch 0 loss at step: 2615000 is 0.12419363032891415\n",
      "epoch 0 loss at step: 2620000 is 0.12065004767165519\n",
      "epoch 0 loss at step: 2625000 is 0.1263918356722221\n",
      "epoch 0 loss at step: 2630000 is 0.12271259479224682\n",
      "epoch 0 loss at step: 2635000 is 0.119679419168178\n",
      "epoch 0 loss at step: 2640000 is 0.12125509033426643\n",
      "epoch 0 loss at step: 2645000 is 0.12416805512085557\n",
      "epoch 0 loss at step: 2650000 is 0.11397713329717517\n",
      "epoch 0 loss at step: 2655000 is 0.11961808860264718\n",
      "epoch 0 loss at step: 2660000 is 0.12141292982324958\n",
      "epoch 0 loss at step: 2665000 is 0.11916310376860201\n",
      "epoch 0 loss at step: 2670000 is 0.12423017443688586\n",
      "epoch 0 loss at step: 2675000 is 0.12271377583742142\n",
      "epoch 0 loss at step: 2680000 is 0.12117341888416558\n",
      "epoch 0 loss at step: 2685000 is 0.11730083943895298\n",
      "epoch 0 loss at step: 2690000 is 0.11804382461234927\n",
      "epoch 0 loss at step: 2695000 is 0.1191974796043709\n",
      "epoch 0 loss at step: 2700000 is 0.12089827241450549\n",
      "epoch 0 loss at step: 2705000 is 0.12138446869680192\n",
      "epoch 0 loss at step: 2710000 is 0.11702385549675673\n",
      "epoch 0 loss at step: 2715000 is 0.1205049050072208\n",
      "epoch 0 loss at step: 2720000 is 0.11948722222410142\n",
      "epoch 0 loss at step: 2725000 is 0.12041534199854359\n",
      "epoch 0 loss at step: 2730000 is 0.12499347942601889\n",
      "epoch 0 loss at step: 2735000 is 0.11350364205613732\n",
      "epoch 0 loss at step: 2740000 is 0.11889616804607213\n",
      "epoch 0 loss at step: 2745000 is 0.12531567508485167\n",
      "epoch 0 loss at step: 2750000 is 0.1205491582436487\n",
      "epoch 0 loss at step: 2755000 is 0.12656823212737217\n",
      "epoch 0 loss at step: 2760000 is 0.11934336674064398\n",
      "epoch 0 loss at step: 2765000 is 0.118551719775144\n",
      "epoch 0 loss at step: 2770000 is 0.12766318695591763\n",
      "epoch 0 loss at step: 2775000 is 0.1193559837244451\n",
      "epoch 0 loss at step: 2780000 is 0.12146906944462098\n",
      "epoch 0 loss at step: 2785000 is 0.11698812318546697\n",
      "epoch 0 loss at step: 2790000 is 0.12101136742066591\n",
      "epoch 0 loss at step: 2795000 is 0.12106553332377225\n",
      "epoch 0 loss at step: 2800000 is 0.1174344754689373\n",
      "epoch 0 loss at step: 2805000 is 0.1258189419928938\n",
      "epoch 0 loss at step: 2810000 is 0.11629315420305357\n",
      "epoch 0 loss at step: 2815000 is 0.11940145372785628\n",
      "epoch 0 loss at step: 2820000 is 0.12029357052147388\n",
      "epoch 0 loss at step: 2825000 is 0.11679323577918112\n",
      "epoch 0 loss at step: 2830000 is 0.12172410019757227\n",
      "epoch 0 loss at step: 2835000 is 0.11900636534774676\n",
      "epoch 0 loss at step: 2840000 is 0.1176164227493573\n",
      "epoch 0 loss at step: 2845000 is 0.12324711249303073\n",
      "epoch 0 loss at step: 2850000 is 0.12109363304842263\n",
      "epoch 0 loss at step: 2855000 is 0.117933318214491\n",
      "epoch 0 loss at step: 2860000 is 0.11863694705879316\n",
      "epoch 0 loss at step: 2865000 is 0.1191735715225339\n",
      "epoch 0 loss at step: 2870000 is 0.11354800748336129\n",
      "epoch 0 loss at step: 2875000 is 0.1172417111958377\n",
      "epoch 0 loss at step: 2880000 is 0.11851879158467055\n",
      "epoch 0 loss at step: 2885000 is 0.11812451067632064\n",
      "epoch 0 loss at step: 2890000 is 0.11763130867294967\n",
      "epoch 0 loss at step: 2895000 is 0.11302641079635359\n",
      "epoch 0 loss at step: 2900000 is 0.10811094828011701\n",
      "epoch 0 loss at step: 2905000 is 0.12571048252638428\n",
      "epoch 0 loss at step: 2910000 is 0.11743670710702427\n",
      "epoch 0 loss at step: 2915000 is 0.1151756574722007\n",
      "epoch 0 loss at step: 2920000 is 0.1178100999649614\n",
      "epoch 0 loss at step: 2925000 is 0.11970843898952008\n",
      "epoch 0 loss at step: 2930000 is 0.11437667609155178\n",
      "epoch 0 loss at step: 2935000 is 0.1187668196760118\n",
      "epoch 0 loss at step: 2940000 is 0.1185315700262785\n",
      "epoch 0 loss at step: 2945000 is 0.11823024968504905\n",
      "epoch 0 loss at step: 2950000 is 0.12475811827983707\n",
      "epoch 0 loss at step: 2955000 is 0.11382358453422785\n",
      "epoch 0 loss at step: 2960000 is 0.12088070273511112\n",
      "epoch 0 loss at step: 2965000 is 0.11974184485040605\n",
      "epoch 0 loss at step: 2970000 is 0.12218386215325445\n",
      "epoch 0 loss at step: 2975000 is 0.11741207912089303\n",
      "epoch 0 loss at step: 2980000 is 0.11951026427410542\n",
      "epoch 0 loss at step: 2985000 is 0.12268508572019636\n",
      "epoch 0 loss at step: 2990000 is 0.12028196843164042\n",
      "epoch 0 loss at step: 2995000 is 0.12018337679766118\n",
      "epoch 0 loss at step: 3000000 is 0.12268670752961189\n",
      "epoch 0 loss at step: 3005000 is 0.12239870103802532\n",
      "epoch 0 loss at step: 3010000 is 0.11402917501004413\n",
      "epoch 0 loss at step: 3015000 is 0.12207438680762425\n",
      "epoch 0 loss at step: 3020000 is 0.11944427683884278\n",
      "epoch 0 loss at step: 3025000 is 0.12217460767962038\n",
      "epoch 0 loss at step: 3030000 is 0.1187445032005664\n",
      "epoch 0 loss at step: 3035000 is 0.115547346921172\n",
      "epoch 0 loss at step: 3040000 is 0.11777643784191459\n",
      "epoch 0 loss at step: 3045000 is 0.12237180132679641\n",
      "epoch 0 loss at step: 3050000 is 0.11980338954543694\n",
      "epoch 0 loss at step: 3055000 is 0.1160490373827517\n",
      "epoch 0 loss at step: 3060000 is 0.11656175046833232\n",
      "epoch 0 loss at step: 3065000 is 0.11657059923680499\n",
      "epoch 0 loss at step: 3070000 is 0.11972850642027333\n",
      "epoch 0 loss at step: 3075000 is 0.11642067092191428\n",
      "epoch 0 loss at step: 3080000 is 0.11827013889672235\n",
      "epoch 0 loss at step: 3085000 is 0.12117036752644926\n",
      "epoch 0 loss at step: 3090000 is 0.11262382253063842\n",
      "epoch 0 loss at step: 3095000 is 0.12131035560565069\n",
      "epoch 0 loss at step: 3100000 is 0.12002162914946675\n",
      "epoch 0 loss at step: 3105000 is 0.12012694405969232\n",
      "epoch 0 loss at step: 3110000 is 0.11726153703182936\n",
      "epoch 0 loss at step: 3115000 is 0.11820716222012416\n",
      "epoch 0 loss at step: 3120000 is 0.11488293714234606\n",
      "epoch 0 loss at step: 3125000 is 0.11646975177358836\n",
      "epoch 0 loss at step: 3130000 is 0.11950405573826284\n",
      "epoch 0 loss at step: 3135000 is 0.11374291792809964\n",
      "epoch 0 loss at step: 3140000 is 0.1198067945559509\n",
      "epoch 0 loss at step: 3145000 is 0.11758907259907574\n",
      "epoch 0 loss at step: 3150000 is 0.12013609813302756\n",
      "epoch 0 loss at step: 3155000 is 0.12336064258050174\n",
      "epoch 0 loss at step: 3160000 is 0.11559369722548872\n",
      "epoch 0 loss at step: 3165000 is 0.12309208092289045\n",
      "epoch 0 loss at step: 3170000 is 0.11617358601801098\n",
      "epoch 0 loss at step: 3175000 is 0.11885881685959175\n",
      "epoch 0 loss at step: 3180000 is 0.12036237324448303\n",
      "epoch 0 loss at step: 3185000 is 0.11467642532531172\n",
      "epoch 0 loss at step: 3190000 is 0.11708363265432417\n",
      "epoch 0 loss at step: 3195000 is 0.12042288035880774\n",
      "epoch 0 loss at step: 3200000 is 0.11778785099992528\n",
      "epoch 0 loss at step: 3205000 is 0.12100863158870488\n",
      "epoch 0 loss at step: 3210000 is 0.12344943680167199\n",
      "epoch 0 loss at step: 3215000 is 0.12147283936776221\n",
      "epoch 0 loss at step: 3220000 is 0.1154902891636826\n",
      "epoch 0 loss at step: 3225000 is 0.11311652766717598\n",
      "epoch 0 loss at step: 3230000 is 0.11735350335249678\n",
      "epoch 0 loss at step: 3235000 is 0.11856973182726652\n",
      "epoch 0 loss at step: 3240000 is 0.11690453422628343\n",
      "epoch 0 loss at step: 3245000 is 0.11851195693090558\n",
      "epoch 0 loss at step: 3250000 is 0.1122652172325179\n",
      "epoch 0 loss at step: 3255000 is 0.12237521914904938\n",
      "epoch 0 loss at step: 3260000 is 0.1179096927685663\n",
      "epoch 0 loss at step: 3265000 is 0.11572685927897691\n",
      "epoch 0 loss at step: 3270000 is 0.11962466552087571\n",
      "epoch 0 loss at step: 3275000 is 0.11742391225481406\n",
      "epoch 0 loss at step: 3280000 is 0.12133754193373024\n",
      "epoch 0 loss at step: 3285000 is 0.11916406963281334\n",
      "epoch 0 loss at step: 3290000 is 0.11709849038082175\n",
      "epoch 0 loss at step: 3295000 is 0.11705853498545475\n",
      "epoch 0 loss at step: 3300000 is 0.11816678959075362\n",
      "epoch 0 loss at step: 3305000 is 0.11822879867777228\n",
      "epoch 0 loss at step: 3310000 is 0.11890408562524245\n",
      "epoch 0 loss at step: 3315000 is 0.11436096821688116\n",
      "epoch 0 loss at step: 3320000 is 0.11846409565415233\n",
      "epoch 0 loss at step: 3325000 is 0.11731213896675036\n",
      "epoch 0 loss at step: 3330000 is 0.11564582455735653\n",
      "epoch 0 loss at step: 3335000 is 0.10952823359398171\n",
      "epoch 0 loss at step: 3340000 is 0.11755918473843485\n",
      "epoch 0 loss at step: 3345000 is 0.11679334748722613\n",
      "epoch 0 loss at step: 3350000 is 0.11577374073006212\n",
      "epoch 0 loss at step: 3355000 is 0.11792490083593875\n",
      "epoch 0 loss at step: 3360000 is 0.11737524991650135\n",
      "epoch 0 loss at step: 3365000 is 0.11778625732790679\n",
      "epoch 0 loss at step: 3370000 is 0.12258926773127168\n",
      "epoch 0 loss at step: 3375000 is 0.11571646748562343\n",
      "epoch 0 loss at step: 3380000 is 0.11211671153493226\n",
      "epoch 0 loss at step: 3385000 is 0.11676968170963228\n",
      "epoch 0 loss at step: 3390000 is 0.11349728034790606\n",
      "epoch 0 loss at step: 3395000 is 0.12165433719847352\n",
      "epoch 0 loss at step: 3400000 is 0.1192598183138296\n",
      "epoch 0 loss at step: 3405000 is 0.11748160496335476\n",
      "epoch 0 loss at step: 3410000 is 0.11891672402611002\n",
      "epoch 0 loss at step: 3415000 is 0.11513665234781802\n",
      "epoch 0 loss at step: 3420000 is 0.1181530835358426\n",
      "epoch 0 loss at step: 3425000 is 0.11531420584693551\n",
      "epoch 0 loss at step: 3430000 is 0.11617544272579253\n",
      "epoch 0 loss at step: 3435000 is 0.12564305431991815\n",
      "epoch 0 loss at step: 3440000 is 0.1146254064437002\n",
      "epoch 0 loss at step: 3445000 is 0.1156886256620288\n",
      "epoch 0 loss at step: 3450000 is 0.11813654901329428\n",
      "epoch 0 loss at step: 3455000 is 0.12075276629058644\n",
      "epoch 0 loss at step: 3460000 is 0.12052881793910637\n",
      "epoch 0 loss at step: 3465000 is 0.11706225424762816\n",
      "epoch 0 loss at step: 3470000 is 0.11921876774560661\n",
      "epoch 0 loss at step: 3475000 is 0.12445373231563717\n",
      "epoch 0 loss at step: 3480000 is 0.11842032621502876\n",
      "epoch 0 loss at step: 3485000 is 0.11655918212272227\n",
      "epoch 0 loss at step: 3490000 is 0.11505984194939956\n",
      "epoch 0 loss at step: 3495000 is 0.11520844677146524\n",
      "epoch 0 loss at step: 3500000 is 0.12501671383138746\n",
      "epoch 0 loss at step: 3505000 is 0.12160944719202817\n",
      "epoch 0 loss at step: 3510000 is 0.12038956331657245\n",
      "epoch 0 loss at step: 3515000 is 0.11499024210986682\n",
      "epoch 0 loss at step: 3520000 is 0.11444020695374348\n",
      "epoch 0 loss at step: 3525000 is 0.11837329126112163\n",
      "epoch 0 loss at step: 3530000 is 0.11601649987101555\n",
      "epoch 0 loss at step: 3535000 is 0.11608833797648549\n",
      "epoch 0 loss at step: 3540000 is 0.12140591345597058\n",
      "epoch 0 loss at step: 3545000 is 0.11684018558585085\n",
      "epoch 0 loss at step: 3550000 is 0.11248733317628502\n",
      "epoch 0 loss at step: 3555000 is 0.11764448996111751\n",
      "epoch 0 loss at step: 3560000 is 0.1198488465514034\n",
      "epoch 0 loss at step: 3565000 is 0.1126365758066997\n",
      "epoch 0 loss at step: 3570000 is 0.11715681046973914\n",
      "epoch 0 loss at step: 3575000 is 0.11471739554647356\n",
      "epoch 0 loss at step: 3580000 is 0.1139039890088141\n",
      "epoch 0 loss at step: 3585000 is 0.11327984915114939\n",
      "epoch 0 loss at step: 3590000 is 0.11210734017696232\n",
      "epoch 0 loss at step: 3595000 is 0.11423818559870123\n",
      "epoch 0 loss at step: 3600000 is 0.1126032567392569\n",
      "epoch 0 loss at step: 3605000 is 0.12085706388223916\n",
      "epoch 0 loss at step: 3610000 is 0.11094837790541351\n",
      "epoch 0 loss at step: 3615000 is 0.11604647314161994\n",
      "epoch 0 loss at step: 3620000 is 0.11424858152456581\n",
      "epoch 0 loss at step: 3625000 is 0.11905609340528026\n",
      "epoch 0 loss at step: 3630000 is 0.12145053101070226\n",
      "epoch 0 loss at step: 3635000 is 0.11586503599369899\n",
      "epoch 0 loss at step: 3640000 is 0.11353284205086529\n",
      "epoch 0 loss at step: 3645000 is 0.11280694262012839\n",
      "epoch 0 loss at step: 3650000 is 0.11582189727714286\n",
      "epoch 0 loss at step: 3655000 is 0.11428430192973464\n",
      "epoch 0 loss at step: 3660000 is 0.11277309956885874\n",
      "epoch 0 loss at step: 3665000 is 0.11404487448222936\n",
      "epoch 0 loss at step: 3670000 is 0.11288621325353161\n",
      "epoch 0 loss at step: 3675000 is 0.1110298589553684\n",
      "epoch 0 loss at step: 3680000 is 0.11440109827840933\n",
      "epoch 0 loss at step: 3685000 is 0.11711504324497655\n",
      "epoch 0 loss at step: 3690000 is 0.11722296928372235\n",
      "epoch 0 loss at step: 3695000 is 0.11118105256436393\n",
      "epoch 0 loss at step: 3700000 is 0.11329606742803007\n",
      "epoch 0 loss at step: 3705000 is 0.11668119921442122\n",
      "epoch 0 loss at step: 3710000 is 0.1118206177290529\n",
      "epoch 0 loss at step: 3715000 is 0.11136031007934362\n",
      "epoch 0 loss at step: 3720000 is 0.11155138600319624\n",
      "epoch 0 loss at step: 3725000 is 0.11248673139568419\n",
      "epoch 0 loss at step: 3730000 is 0.11658796159420162\n",
      "epoch 0 loss at step: 3735000 is 0.11919809809885919\n",
      "epoch 0 loss at step: 3740000 is 0.11146855880059302\n",
      "epoch 0 loss at step: 3745000 is 0.11674870327673853\n",
      "epoch 0 loss at step: 3750000 is 0.1137846488902811\n",
      "epoch 0 loss at step: 3755000 is 0.112298698682338\n",
      "epoch 0 loss at step: 3760000 is 0.11978827651981265\n",
      "epoch 0 loss at step: 3765000 is 0.1205600271188654\n",
      "epoch 0 loss at step: 3770000 is 0.11381853369348682\n",
      "epoch 0 loss at step: 3775000 is 0.11600606527021155\n",
      "epoch 0 loss at step: 3780000 is 0.11610258884886279\n",
      "epoch 0 loss at step: 3785000 is 0.1132848935097456\n",
      "epoch 0 loss at step: 3790000 is 0.12162517732074485\n",
      "epoch 0 loss at step: 3795000 is 0.11602907437141985\n",
      "epoch 0 loss at step: 3800000 is 0.11530660107079893\n",
      "epoch 0 loss at step: 3805000 is 0.11810215348619968\n",
      "epoch 0 loss at step: 3810000 is 0.10943801263505593\n",
      "epoch 0 loss at step: 3815000 is 0.1176082003337331\n",
      "epoch 0 loss at step: 3820000 is 0.12214504913347773\n",
      "epoch 0 loss at step: 3825000 is 0.11154414288029074\n",
      "epoch 0 loss at step: 3830000 is 0.11826351876473054\n",
      "epoch 0 loss at step: 3835000 is 0.11568820168953389\n",
      "epoch 0 loss at step: 3840000 is 0.11753163682138547\n",
      "epoch 0 loss at step: 3845000 is 0.10956740238694473\n",
      "epoch 0 loss at step: 3850000 is 0.11723757236534729\n",
      "epoch 0 loss at step: 3855000 is 0.11284962147586047\n",
      "epoch 0 loss at step: 3860000 is 0.11083852279395796\n",
      "epoch 0 loss at step: 3865000 is 0.11506563037112355\n",
      "epoch 0 loss at step: 3870000 is 0.11019933152385056\n",
      "epoch 0 loss at step: 3875000 is 0.1147267263953574\n",
      "epoch 0 loss at step: 3880000 is 0.11059482705742121\n",
      "epoch 0 loss at step: 3885000 is 0.11904996339343488\n",
      "epoch 0 loss at step: 3890000 is 0.11968551730960608\n",
      "epoch 0 loss at step: 3895000 is 0.11824930266216398\n",
      "epoch 0 loss at step: 3900000 is 0.11336069232271984\n",
      "epoch 0 loss at step: 3905000 is 0.11821243768092245\n",
      "epoch 0 loss at step: 3910000 is 0.11326468509845436\n",
      "epoch 0 loss at step: 3915000 is 0.11372093040514737\n",
      "epoch 0 loss at step: 3920000 is 0.11440199246257543\n",
      "epoch 0 loss at step: 3925000 is 0.12173730799630285\n",
      "epoch 0 loss at step: 3930000 is 0.11030803835112601\n",
      "epoch 0 loss at step: 3935000 is 0.10537090549152345\n",
      "epoch 0 loss at step: 3940000 is 0.11829642483452335\n",
      "epoch 0 loss at step: 3945000 is 0.11591337634194643\n",
      "epoch 0 loss at step: 3950000 is 0.11563319455590099\n",
      "epoch 0 loss at step: 3955000 is 0.11404369293414056\n",
      "epoch 0 loss at step: 3960000 is 0.11586120027061551\n",
      "epoch 0 loss at step: 3965000 is 0.11225031166467815\n",
      "epoch 0 loss at step: 3970000 is 0.1213106479395181\n",
      "epoch 0 loss at step: 3975000 is 0.11307973814411089\n",
      "epoch 0 loss at step: 3980000 is 0.11417052049282939\n",
      "epoch 0 loss at step: 3985000 is 0.11217674520555884\n",
      "epoch 0 loss at step: 3990000 is 0.11863641589693726\n",
      "epoch 0 loss at step: 3995000 is 0.11277196378614754\n",
      "epoch 0 loss at step: 4000000 is 0.11801221490837634\n",
      "epoch 0 loss at step: 4005000 is 0.11570505565200001\n",
      "epoch 0 loss at step: 4010000 is 0.11429422815879807\n",
      "epoch 0 loss at step: 4015000 is 0.11506577686639502\n",
      "epoch 0 loss at step: 4020000 is 0.11026421287632547\n",
      "epoch 0 loss at step: 4025000 is 0.11667226760052145\n",
      "epoch 0 loss at step: 4030000 is 0.11224270771932789\n",
      "epoch 0 loss at step: 4035000 is 0.11538767298045569\n",
      "epoch 0 loss at step: 4040000 is 0.11071601777113974\n",
      "epoch 0 loss at step: 4045000 is 0.11250480552734807\n",
      "epoch 0 loss at step: 4050000 is 0.11026486383602023\n",
      "epoch 0 loss at step: 4055000 is 0.1142683867348358\n",
      "epoch 0 loss at step: 4060000 is 0.11568085656389594\n",
      "epoch 0 loss at step: 4065000 is 0.10761483772941866\n",
      "epoch 0 loss at step: 4070000 is 0.11164758602827787\n",
      "epoch 0 loss at step: 4075000 is 0.11216842169705778\n",
      "epoch 0 loss at step: 4080000 is 0.11222272467780858\n",
      "epoch 0 loss at step: 4085000 is 0.11584444685168564\n",
      "epoch 0 loss at step: 4090000 is 0.11575048296386375\n",
      "epoch 0 loss at step: 4095000 is 0.11327895594052971\n",
      "epoch 0 loss at step: 4100000 is 0.1112106195711065\n",
      "epoch 0 loss at step: 4105000 is 0.1125441993067041\n",
      "epoch 0 loss at step: 4110000 is 0.11983053622981533\n",
      "epoch 0 loss at step: 4115000 is 0.11520664131073281\n",
      "epoch 0 loss at step: 4120000 is 0.11365339664528146\n",
      "epoch 0 loss at step: 4125000 is 0.11394029774330557\n",
      "epoch 0 loss at step: 4130000 is 0.11526158002102747\n",
      "epoch 0 loss at step: 4135000 is 0.11628904672116042\n",
      "epoch 0 loss at step: 4140000 is 0.11291340707521885\n",
      "epoch 0 loss at step: 4145000 is 0.11471220573498868\n",
      "epoch 0 loss at step: 4150000 is 0.11604991341158748\n",
      "epoch 0 loss at step: 4155000 is 0.115552807018999\n",
      "epoch 0 loss at step: 4160000 is 0.12082064761519432\n",
      "epoch 0 loss at step: 4165000 is 0.11226466620122083\n",
      "epoch 0 loss at step: 4170000 is 0.11752616187464446\n",
      "epoch 0 loss at step: 4175000 is 0.11211076384242624\n",
      "epoch 0 loss at step: 4180000 is 0.11587965967096388\n",
      "epoch 0 loss at step: 4185000 is 0.11191177617360955\n",
      "epoch 0 loss at step: 4190000 is 0.1173299601290375\n",
      "epoch 0 loss at step: 4195000 is 0.116474816512689\n",
      "epoch 0 loss at step: 4200000 is 0.11518412997741252\n",
      "epoch 0 loss at step: 4205000 is 0.11487755332943052\n",
      "epoch 0 loss at step: 4210000 is 0.11893275411035865\n",
      "epoch 0 loss at step: 4215000 is 0.11528246089410968\n",
      "epoch 0 loss at step: 4220000 is 0.11924311122044455\n",
      "epoch 0 loss at step: 4225000 is 0.11505440690973774\n",
      "epoch 0 loss at step: 4230000 is 0.11587458916045726\n",
      "epoch 0 loss at step: 4235000 is 0.10772855433840305\n",
      "epoch 0 loss at step: 4240000 is 0.12097918953001499\n",
      "epoch 0 loss at step: 4245000 is 0.11506394189642742\n",
      "epoch 0 loss at step: 4250000 is 0.11123871050355956\n",
      "epoch 0 loss at step: 4255000 is 0.1156914477549959\n",
      "epoch 0 loss at step: 4260000 is 0.121388203250058\n",
      "epoch 0 loss at step: 4265000 is 0.11008707428360358\n",
      "epoch 0 loss at step: 4270000 is 0.11200647955322639\n",
      "epoch 0 loss at step: 4275000 is 0.11640564185027033\n",
      "epoch 0 loss at step: 4280000 is 0.11771911459118128\n",
      "epoch 0 loss at step: 4285000 is 0.11690802996326238\n",
      "epoch 0 loss at step: 4290000 is 0.11230566690191626\n",
      "epoch 0 loss at step: 4295000 is 0.11422427377179264\n",
      "epoch 0 loss at step: 4300000 is 0.11044198244065047\n",
      "epoch 0 loss at step: 4305000 is 0.10888276132140308\n",
      "epoch 0 loss at step: 4310000 is 0.1104392026528716\n",
      "epoch 0 loss at step: 4315000 is 0.11208996764319018\n",
      "epoch 0 loss at step: 4320000 is 0.11550055654030293\n",
      "epoch 0 loss at step: 4325000 is 0.1111727656122297\n",
      "epoch 0 loss at step: 4330000 is 0.11133966703868936\n",
      "epoch 0 loss at step: 4335000 is 0.11782144109308719\n",
      "epoch 0 loss at step: 4340000 is 0.11127402153192088\n",
      "epoch 0 loss at step: 4345000 is 0.11663880755659192\n",
      "epoch 0 loss at step: 4350000 is 0.11635413033058867\n",
      "epoch 0 loss at step: 4355000 is 0.10859427631758153\n",
      "epoch 0 loss at step: 4360000 is 0.1153694846331142\n",
      "epoch 0 loss at step: 4365000 is 0.11238251749854535\n",
      "epoch 0 loss at step: 4370000 is 0.1112213117338717\n",
      "epoch 0 loss at step: 4375000 is 0.11287318334681913\n",
      "epoch 0 loss at step: 4380000 is 0.11105276844911277\n",
      "epoch 0 loss at step: 4385000 is 0.11300215378552675\n",
      "epoch 0 loss at step: 4390000 is 0.1103702854141593\n",
      "epoch 0 loss at step: 4395000 is 0.11663337993845344\n",
      "epoch 0 loss at step: 4400000 is 0.11107758223221172\n",
      "epoch 0 loss at step: 4405000 is 0.11284398672869429\n",
      "epoch 0 loss at step: 4410000 is 0.11396295791976153\n",
      "epoch 0 loss at step: 4415000 is 0.10990417332234792\n",
      "epoch 0 loss at step: 4420000 is 0.11457669689003379\n",
      "epoch 0 loss at step: 4425000 is 0.11018483849838376\n",
      "epoch 0 loss at step: 4430000 is 0.11515449912883341\n",
      "epoch 0 loss at step: 4435000 is 0.11418688138304278\n",
      "epoch 0 loss at step: 4440000 is 0.11986305987164378\n",
      "epoch 0 loss at step: 4445000 is 0.1107553905248642\n",
      "epoch 0 loss at step: 4450000 is 0.11637675289157778\n",
      "epoch 0 loss at step: 4455000 is 0.11474949936121702\n",
      "epoch 0 loss at step: 4460000 is 0.11391465327907353\n",
      "epoch 0 loss at step: 4465000 is 0.11625417334567756\n",
      "epoch 0 loss at step: 4470000 is 0.10932674615681172\n",
      "epoch 0 loss at step: 4475000 is 0.11276412113066762\n",
      "epoch 0 loss at step: 4480000 is 0.11474024933222682\n",
      "epoch 0 loss at step: 4485000 is 0.11742973219826817\n",
      "epoch 0 loss at step: 4490000 is 0.11340452582638245\n",
      "epoch 0 loss at step: 4495000 is 0.10759473016234115\n",
      "epoch 0 loss at step: 4500000 is 0.11648187742233276\n",
      "epoch 0 loss at step: 4505000 is 0.11466636321814731\n",
      "epoch 0 loss at step: 4510000 is 0.10986233728313818\n",
      "epoch 0 loss at step: 4515000 is 0.11414282241910696\n",
      "epoch 0 loss at step: 4520000 is 0.1126967555264011\n",
      "epoch 0 loss at step: 4525000 is 0.11358847140856088\n",
      "epoch 0 loss at step: 4530000 is 0.10922880550287664\n",
      "epoch 0 loss at step: 4535000 is 0.11269075868092478\n",
      "epoch 0 loss at step: 4540000 is 0.11190874324524776\n",
      "epoch 0 loss at step: 4545000 is 0.11201198002230375\n",
      "epoch 0 loss at step: 4550000 is 0.11270633853096515\n",
      "epoch 0 loss at step: 4555000 is 0.10862690500933676\n",
      "epoch 0 loss at step: 4560000 is 0.10641947425352409\n",
      "epoch 0 loss at step: 4565000 is 0.11270600252337754\n",
      "epoch 0 loss at step: 4570000 is 0.1090942004673183\n",
      "epoch 0 loss at step: 4575000 is 0.10579616254866123\n",
      "epoch 0 loss at step: 4580000 is 0.1104697901159525\n",
      "epoch 0 loss at step: 4585000 is 0.10729550964348018\n",
      "epoch 0 loss at step: 4590000 is 0.10933856899775565\n",
      "epoch 0 loss at step: 4595000 is 0.1134177970613353\n",
      "epoch 0 loss at step: 4600000 is 0.11087268602401018\n",
      "epoch 0 loss at step: 4605000 is 0.10954870186327026\n",
      "epoch 0 loss at step: 4610000 is 0.11239881258094683\n",
      "epoch 0 loss at step: 4615000 is 0.11102484908923507\n",
      "epoch 0 loss at step: 4620000 is 0.11228604305339977\n",
      "epoch 0 loss at step: 4625000 is 0.11217672933870927\n",
      "epoch 0 loss at step: 4630000 is 0.11406052994243801\n",
      "epoch 0 loss at step: 4635000 is 0.11329303826242686\n",
      "epoch 0 loss at step: 4640000 is 0.10531440662723035\n",
      "epoch 0 loss at step: 4645000 is 0.1079253711479716\n",
      "epoch 0 loss at step: 4650000 is 0.11019220362864435\n",
      "epoch 0 loss at step: 4655000 is 0.1136604002535576\n",
      "epoch 0 loss at step: 4660000 is 0.10543559889607132\n",
      "epoch 0 loss at step: 4665000 is 0.11502048854483292\n",
      "epoch 0 loss at step: 4670000 is 0.11119389996845275\n",
      "epoch 0 loss at step: 4675000 is 0.11130494259502739\n",
      "epoch 0 loss at step: 4680000 is 0.11276784665221348\n",
      "epoch 0 loss at step: 4685000 is 0.11736768622044474\n",
      "epoch 0 loss at step: 4690000 is 0.11196649343036115\n",
      "epoch 0 loss at step: 4695000 is 0.11142608257057145\n",
      "epoch 0 loss at step: 4700000 is 0.11819662295626476\n",
      "epoch 0 loss at step: 4705000 is 0.10624533239332959\n",
      "epoch 0 loss at step: 4710000 is 0.11006979433707893\n",
      "epoch 0 loss at step: 4715000 is 0.11534671841105447\n",
      "epoch 0 loss at step: 4720000 is 0.11730872772429138\n",
      "epoch 0 loss at step: 4725000 is 0.1114113014887087\n",
      "epoch 0 loss at step: 4730000 is 0.11491821315318812\n",
      "epoch 0 loss at step: 4735000 is 0.11280484241675585\n",
      "epoch 0 loss at step: 4740000 is 0.1111263329830952\n",
      "epoch 0 loss at step: 4745000 is 0.11213294423762708\n",
      "epoch 0 loss at step: 4750000 is 0.11623835874907672\n",
      "epoch 0 loss at step: 4755000 is 0.1110036230194848\n",
      "epoch 0 loss at step: 4760000 is 0.10815785750641953\n",
      "epoch 0 loss at step: 4765000 is 0.11145562863890082\n",
      "epoch 0 loss at step: 4770000 is 0.11040367287397385\n",
      "epoch 0 loss at step: 4775000 is 0.1104758849779144\n",
      "epoch 0 loss at step: 4780000 is 0.1129252640068531\n",
      "epoch 0 loss at step: 4785000 is 0.1161551818984095\n",
      "epoch 0 loss at step: 4790000 is 0.11379848110051825\n",
      "epoch 0 loss at step: 4795000 is 0.10824463605228811\n",
      "epoch 0 loss at step: 4800000 is 0.1134945062007755\n",
      "epoch 0 loss at step: 4805000 is 0.11629042323813774\n",
      "epoch 0 loss at step: 4810000 is 0.11045380460983142\n",
      "epoch 0 loss at step: 4815000 is 0.10949325507637113\n",
      "epoch 0 loss at step: 4820000 is 0.10728230956960469\n",
      "epoch 0 loss at step: 4825000 is 0.11067543107252568\n",
      "epoch 0 loss at step: 4830000 is 0.11497878254419193\n",
      "epoch 0 loss at step: 4835000 is 0.10744943092819303\n",
      "epoch 0 loss at step: 4840000 is 0.10847871331910137\n",
      "epoch 0 loss at step: 4845000 is 0.10985621594395488\n",
      "epoch 0 loss at step: 4850000 is 0.10616899049240164\n",
      "epoch 0 loss at step: 4855000 is 0.1128579376835376\n",
      "epoch 0 loss at step: 4860000 is 0.11346464057788253\n",
      "epoch 0 loss at step: 4865000 is 0.10770308897411451\n",
      "epoch 0 loss at step: 4870000 is 0.10875726290941239\n",
      "epoch 0 loss at step: 4875000 is 0.11125405493956059\n",
      "epoch 0 loss at step: 4880000 is 0.11386948530003428\n",
      "epoch 0 loss at step: 4885000 is 0.11205702567622065\n",
      "epoch 0 loss at step: 4890000 is 0.113868786576204\n",
      "epoch 0 loss at step: 4895000 is 0.112542938054353\n",
      "epoch 0 loss at step: 4900000 is 0.11599660866679623\n",
      "epoch 0 loss at step: 4905000 is 0.1095816274933517\n",
      "epoch 0 loss at step: 4910000 is 0.11456316888146102\n",
      "epoch 0 loss at step: 4915000 is 0.11704175644288771\n",
      "epoch 0 loss at step: 4920000 is 0.11397833849573508\n",
      "epoch 0 loss at step: 4925000 is 0.11286146572455764\n",
      "epoch 0 loss at step: 4930000 is 0.10762213189965114\n",
      "epoch 0 loss at step: 4935000 is 0.11348151631923392\n",
      "epoch 0 loss at step: 4940000 is 0.11051076425407082\n",
      "epoch 0 loss at step: 4945000 is 0.11295917576774955\n",
      "epoch 0 loss at step: 4950000 is 0.11671339254472404\n",
      "epoch 0 loss at step: 4955000 is 0.10303772209137678\n",
      "epoch 0 loss at step: 4960000 is 0.10706207509534434\n",
      "epoch 0 loss at step: 4965000 is 0.1109030084149912\n",
      "epoch 0 loss at step: 4970000 is 0.10672931551756337\n",
      "epoch 0 loss at step: 4975000 is 0.11040364410253242\n",
      "epoch 0 loss at step: 4980000 is 0.10948432493153959\n",
      "epoch 0 loss at step: 4985000 is 0.11485457282550633\n",
      "epoch 0 loss at step: 4990000 is 0.11045384558402002\n",
      "epoch 0 loss at step: 4995000 is 0.10908225314849988\n",
      "epoch 0 loss at step: 5000000 is 0.10856294695287944\n",
      "epoch 0 loss at step: 5005000 is 0.11506758325467817\n",
      "epoch 0 loss at step: 5010000 is 0.11073772697392852\n",
      "epoch 0 loss at step: 5015000 is 0.10833423308301716\n",
      "epoch 0 loss at step: 5020000 is 0.11478297542389482\n",
      "epoch 0 loss at step: 5025000 is 0.11217411646889523\n",
      "epoch 0 loss at step: 5030000 is 0.10933638714887202\n",
      "epoch 0 loss at step: 5035000 is 0.10599529617829248\n",
      "epoch 0 loss at step: 5040000 is 0.11275199004821479\n",
      "epoch 0 loss at step: 5045000 is 0.11322686494598165\n",
      "epoch 0 loss at step: 5050000 is 0.11398860102081672\n",
      "epoch 0 loss at step: 5055000 is 0.11349515271289275\n",
      "epoch 0 loss at step: 5060000 is 0.1077270605427213\n",
      "epoch 0 loss at step: 5065000 is 0.11154524724930524\n",
      "epoch 0 loss at step: 5070000 is 0.10987126489290967\n",
      "epoch 0 loss at step: 5075000 is 0.10939340953691863\n",
      "epoch 0 loss at step: 5080000 is 0.10811813019486144\n",
      "epoch 0 loss at step: 5085000 is 0.11532029307028278\n",
      "epoch 0 loss at step: 5090000 is 0.10594965751022101\n",
      "epoch 0 loss at step: 5095000 is 0.10851515980660915\n",
      "epoch 0 loss at step: 5100000 is 0.11248657331410795\n",
      "epoch 0 loss at step: 5105000 is 0.1075503745588474\n",
      "epoch 0 loss at step: 5110000 is 0.11555288492180407\n",
      "epoch 0 loss at step: 5115000 is 0.1115035808592569\n",
      "epoch 0 loss at step: 5120000 is 0.11601998824542388\n",
      "epoch 0 loss at step: 5125000 is 0.10832377857619431\n",
      "epoch 0 loss at step: 5130000 is 0.10835290738753975\n",
      "epoch 0 loss at step: 5135000 is 0.11223036747872829\n",
      "epoch 0 loss at step: 5140000 is 0.10774727493254468\n",
      "epoch 0 loss at step: 5145000 is 0.11084996666572988\n",
      "epoch 0 loss at step: 5150000 is 0.10487277735201642\n",
      "epoch 0 loss at step: 5155000 is 0.11014956014044583\n",
      "epoch 0 loss at step: 5160000 is 0.11053978049382568\n",
      "epoch 0 loss at step: 5165000 is 0.11176949888723903\n",
      "epoch 0 loss at step: 5170000 is 0.10615448989709839\n",
      "epoch 0 loss at step: 5175000 is 0.11566556419413537\n",
      "epoch 0 loss at step: 5180000 is 0.11046827473621816\n",
      "epoch 0 loss at step: 5185000 is 0.10686135232187807\n",
      "epoch 0 loss at step: 5190000 is 0.10571266851453111\n",
      "epoch 0 loss at step: 5195000 is 0.10838542247600853\n",
      "epoch 0 loss at step: 5200000 is 0.11178841539118439\n",
      "epoch 0 loss at step: 5205000 is 0.11419657764229924\n",
      "epoch 0 loss at step: 5210000 is 0.11219531981777109\n",
      "epoch 0 loss at step: 5215000 is 0.11366400309561286\n",
      "epoch 0 loss at step: 5220000 is 0.1130894475704059\n",
      "epoch 0 loss at step: 5225000 is 0.10744352273270488\n",
      "epoch 0 loss at step: 5230000 is 0.10670977125819772\n",
      "epoch 0 loss at step: 5235000 is 0.10625131164025516\n",
      "epoch 0 loss at step: 5240000 is 0.10869340860713274\n",
      "epoch 0 loss at step: 5245000 is 0.10450234722141176\n",
      "epoch 0 loss at step: 5250000 is 0.1092996801411733\n",
      "epoch 0 loss at step: 5255000 is 0.11467997334888205\n",
      "epoch 0 loss at step: 5260000 is 0.10989646397568285\n",
      "epoch 0 loss at step: 5265000 is 0.10656266126502305\n",
      "epoch 0 loss at step: 5270000 is 0.10837716254070401\n",
      "epoch 0 loss at step: 5275000 is 0.10797058325912803\n",
      "epoch 0 loss at step: 5280000 is 0.1135084994996665\n",
      "epoch 0 loss at step: 5285000 is 0.10970539611196145\n",
      "epoch 0 loss at step: 5290000 is 0.10594238320738077\n",
      "epoch 0 loss at step: 5295000 is 0.11192237946614624\n",
      "epoch 0 loss at step: 5300000 is 0.10786832102108747\n",
      "epoch 0 loss at step: 5305000 is 0.10794701984133571\n",
      "epoch 0 loss at step: 5310000 is 0.110205747291632\n",
      "epoch 0 loss at step: 5315000 is 0.11155184947960078\n",
      "epoch 0 loss at step: 5320000 is 0.11124896137290634\n",
      "epoch 0 loss at step: 5325000 is 0.11308549421476201\n",
      "epoch 0 loss at step: 5330000 is 0.10812666079476475\n",
      "epoch 0 loss at step: 5335000 is 0.10742232467643917\n",
      "epoch 0 loss at step: 5340000 is 0.11507155874068849\n",
      "epoch 0 loss at step: 5345000 is 0.10737175916293636\n",
      "epoch 0 loss at step: 5350000 is 0.1083346649998799\n",
      "epoch 0 loss at step: 5355000 is 0.11131703402623534\n",
      "epoch 0 loss at step: 5360000 is 0.10805727981729434\n",
      "epoch 0 loss at step: 5365000 is 0.11430588812306523\n",
      "epoch 0 loss at step: 5370000 is 0.10759049901808612\n",
      "epoch 0 loss at step: 5375000 is 0.11207613015891985\n",
      "epoch 0 loss at step: 5380000 is 0.11198160188123583\n",
      "epoch 0 loss at step: 5385000 is 0.11356145394500344\n",
      "epoch 0 loss at step: 5390000 is 0.10883072605915367\n",
      "epoch 0 loss at step: 5395000 is 0.1077317904850468\n",
      "epoch 0 loss at step: 5400000 is 0.10570328162014485\n",
      "epoch 0 loss at step: 5405000 is 0.10891721542202867\n",
      "epoch 0 loss at step: 5410000 is 0.10931596205076202\n",
      "epoch 0 loss at step: 5415000 is 0.105960964652244\n",
      "epoch 0 loss at step: 5420000 is 0.11193483024369925\n",
      "epoch 0 loss at step: 5425000 is 0.10759561596660643\n",
      "epoch 0 loss at step: 5430000 is 0.10561880431454629\n",
      "epoch 0 loss at step: 5435000 is 0.11256458899839782\n",
      "epoch 0 loss at step: 5440000 is 0.10651876547709108\n",
      "epoch 0 loss at step: 5445000 is 0.10671036530146376\n",
      "epoch 0 loss at step: 5450000 is 0.106741549624037\n",
      "epoch 0 loss at step: 5455000 is 0.11074651966327802\n",
      "epoch 0 loss at step: 5460000 is 0.10868424409357831\n",
      "epoch 0 loss at step: 5465000 is 0.11794208419229835\n",
      "epoch 0 loss at step: 5470000 is 0.1115748106469633\n",
      "epoch 0 loss at step: 5475000 is 0.10795567939309403\n",
      "epoch 0 loss at step: 5480000 is 0.11307676445562392\n",
      "epoch 0 loss at step: 5485000 is 0.10838691247273237\n",
      "epoch 0 loss at step: 5490000 is 0.11150473421104252\n",
      "epoch 0 loss at step: 5495000 is 0.11048670853283256\n",
      "epoch 0 loss at step: 5500000 is 0.11312398555047111\n",
      "epoch 0 loss at step: 5505000 is 0.10744690560195595\n",
      "epoch 0 loss at step: 5510000 is 0.10989123150510713\n",
      "epoch 0 loss at step: 5515000 is 0.1141933794407174\n",
      "epoch 0 loss at step: 5520000 is 0.11060770515534095\n",
      "epoch 0 loss at step: 5525000 is 0.10889819100690075\n",
      "epoch 0 loss at step: 5530000 is 0.11148661209801213\n",
      "epoch 0 loss at step: 5535000 is 0.11575053246747702\n",
      "epoch 0 loss at step: 5540000 is 0.11213163330815733\n",
      "epoch 0 loss at step: 5545000 is 0.10293231714987196\n",
      "epoch 0 loss at step: 5550000 is 0.10636415229253471\n",
      "epoch 0 loss at step: 5555000 is 0.10546103394664824\n",
      "epoch 0 loss at step: 5560000 is 0.11242350287856534\n",
      "epoch 0 loss at step: 5565000 is 0.10535755782164634\n",
      "epoch 0 loss at step: 5570000 is 0.11037113912347704\n",
      "epoch 0 loss at step: 5575000 is 0.11099230262925848\n",
      "epoch 0 loss at step: 5580000 is 0.11306043377555906\n",
      "epoch 0 loss at step: 5585000 is 0.10932016759105027\n",
      "epoch 0 loss at step: 5590000 is 0.10682104680668562\n",
      "epoch 0 loss at step: 5595000 is 0.10856964852921665\n",
      "epoch 0 loss at step: 5600000 is 0.11420372691377997\n",
      "epoch 0 loss at step: 5605000 is 0.10834664881527424\n",
      "epoch 0 loss at step: 5610000 is 0.10767900995817035\n",
      "epoch 0 loss at step: 5615000 is 0.10961534753069281\n",
      "epoch 0 loss at step: 5620000 is 0.10606931729763747\n",
      "epoch 0 loss at step: 5625000 is 0.11498334536673502\n",
      "epoch 0 loss at step: 5630000 is 0.1104224395182915\n",
      "epoch 0 loss at step: 5635000 is 0.11017896984815598\n",
      "epoch 0 loss at step: 5640000 is 0.11648257617019117\n",
      "epoch 0 loss at step: 5645000 is 0.10751754295676946\n",
      "epoch 0 loss at step: 5650000 is 0.11255445296438411\n",
      "epoch 0 loss at step: 5655000 is 0.10713736983481795\n",
      "epoch 0 loss at step: 5660000 is 0.1028112693592906\n",
      "epoch 0 loss at step: 5665000 is 0.11908483730936423\n",
      "epoch 0 loss at step: 5670000 is 0.10639537466801703\n",
      "epoch 0 loss at step: 5675000 is 0.10808503994438798\n",
      "epoch 0 loss at step: 5680000 is 0.11415347279105335\n",
      "epoch 0 loss at step: 5685000 is 0.11142464884836227\n",
      "epoch 0 loss at step: 5690000 is 0.10328694936437532\n",
      "epoch 0 loss at step: 5695000 is 0.10841319217290729\n",
      "epoch 0 loss at step: 5700000 is 0.10619473700467497\n",
      "epoch 0 loss at step: 5705000 is 0.10651006234670057\n",
      "epoch 0 loss at step: 5710000 is 0.10859329823609441\n",
      "epoch 0 loss at step: 5715000 is 0.10594668905539438\n",
      "epoch 0 loss at step: 5720000 is 0.10234031655509025\n",
      "epoch 0 loss at step: 5725000 is 0.1122360256263055\n",
      "epoch 0 loss at step: 5730000 is 0.10764283675532788\n",
      "epoch 0 loss at step: 5735000 is 0.11157588275531306\n",
      "epoch 0 loss at step: 5740000 is 0.10794442232083529\n",
      "epoch 0 loss at step: 5745000 is 0.10795407058261335\n",
      "epoch 0 loss at step: 5750000 is 0.10723379915137775\n",
      "epoch 0 loss at step: 5755000 is 0.11469445045916364\n",
      "epoch 0 loss at step: 5760000 is 0.11056835677064955\n",
      "epoch 0 loss at step: 5765000 is 0.11081936019286513\n",
      "epoch 0 loss at step: 5770000 is 0.10912403590790927\n",
      "epoch 0 loss at step: 5775000 is 0.1067818820098415\n",
      "epoch 0 loss at step: 5780000 is 0.11831064787041395\n",
      "epoch 0 loss at step: 5785000 is 0.11076290701376275\n",
      "epoch 0 loss at step: 5790000 is 0.10795991799663752\n",
      "epoch 0 loss at step: 5795000 is 0.11229419307019561\n",
      "epoch 0 loss at step: 5800000 is 0.10507294766446576\n",
      "epoch 0 loss at step: 5805000 is 0.11000339706577361\n",
      "epoch 0 loss at step: 5810000 is 0.11042642198167742\n",
      "epoch 0 loss at step: 5815000 is 0.107524976622127\n",
      "epoch 0 loss at step: 5820000 is 0.11190060057789088\n",
      "epoch 0 loss at step: 5825000 is 0.10716980419871397\n",
      "epoch 0 loss at step: 5830000 is 0.11188064702479168\n",
      "epoch 0 loss at step: 5835000 is 0.10881747453901917\n",
      "epoch 0 loss at step: 5840000 is 0.11211678173616528\n",
      "epoch 0 loss at step: 5845000 is 0.1015074250546284\n",
      "epoch 0 loss at step: 5850000 is 0.11355331473657862\n",
      "epoch 0 loss at step: 5855000 is 0.10605752953793854\n",
      "epoch 0 loss at step: 5860000 is 0.1111762778583914\n",
      "epoch 0 loss at step: 5865000 is 0.11131221772283316\n",
      "epoch 0 loss at step: 5870000 is 0.11081100134989247\n",
      "epoch 0 loss at step: 5875000 is 0.11590000369381159\n",
      "epoch 0 loss at step: 5880000 is 0.1147293189458549\n",
      "epoch 0 loss at step: 5885000 is 0.10981126746525988\n",
      "epoch 0 loss at step: 5890000 is 0.10622660153927281\n",
      "epoch 0 loss at step: 5895000 is 0.10981172632249073\n",
      "epoch 0 loss at step: 5900000 is 0.11100363690908999\n",
      "epoch 0 loss at step: 5905000 is 0.10896509421784431\n",
      "epoch 0 loss at step: 5910000 is 0.11198732965067029\n",
      "epoch 0 loss at step: 5915000 is 0.11111684847120196\n",
      "epoch 0 loss at step: 5920000 is 0.10859699900764971\n",
      "epoch 0 loss at step: 5925000 is 0.10643896398963407\n",
      "epoch 0 loss at step: 5930000 is 0.10793779268153011\n",
      "epoch 0 loss at step: 5935000 is 0.10868611276475712\n",
      "epoch 0 loss at step: 5940000 is 0.10760643067578785\n",
      "epoch 0 loss at step: 5945000 is 0.1103317473848816\n",
      "epoch 0 loss at step: 5950000 is 0.11337517334464937\n",
      "epoch 0 loss at step: 5955000 is 0.11053124885056168\n",
      "epoch 0 loss at step: 5960000 is 0.10802075923057274\n",
      "epoch 0 loss at step: 5965000 is 0.10462893339172005\n",
      "epoch 0 loss at step: 5970000 is 0.1135047896090895\n",
      "epoch 0 loss at step: 5975000 is 0.10617431631265208\n",
      "epoch 0 loss at step: 5980000 is 0.11178734613293782\n",
      "epoch 0 loss at step: 5985000 is 0.10396738267196343\n",
      "epoch 0 loss at step: 5990000 is 0.10661535294046626\n",
      "epoch 0 loss at step: 5995000 is 0.11053044390175491\n",
      "epoch 0 loss at step: 6000000 is 0.10971211902135983\n",
      "epoch 0 loss at step: 6005000 is 0.10555459681404755\n",
      "epoch 0 loss at step: 6010000 is 0.10713715784717351\n",
      "epoch 0 loss at step: 6015000 is 0.11234133042450994\n",
      "epoch 0 loss at step: 6020000 is 0.10888910479862243\n",
      "epoch 0 loss at step: 6025000 is 0.11123798639010637\n",
      "epoch 0 loss at step: 6030000 is 0.10843933847928419\n",
      "epoch 0 loss at step: 6035000 is 0.10921270529758186\n",
      "epoch 0 loss at step: 6040000 is 0.11310864237165079\n",
      "epoch 0 loss at step: 6045000 is 0.10677004445446656\n",
      "epoch 0 loss at step: 6050000 is 0.10749336874817964\n",
      "epoch 0 loss at step: 6055000 is 0.10850699479607866\n",
      "epoch 0 loss at step: 6060000 is 0.10812219005152583\n",
      "epoch 0 loss at step: 6065000 is 0.1093892613842152\n",
      "epoch 0 loss at step: 6070000 is 0.10445587037652732\n",
      "epoch 0 loss at step: 6075000 is 0.10949534469540231\n",
      "epoch 0 loss at step: 6080000 is 0.10022343305656686\n",
      "epoch 0 loss at step: 6085000 is 0.10920423304513097\n",
      "epoch 0 loss at step: 6090000 is 0.11153650421034544\n",
      "epoch 0 loss at step: 6095000 is 0.10820011042114347\n",
      "epoch 0 loss at step: 6100000 is 0.10926618421841412\n",
      "epoch 0 loss at step: 6105000 is 0.11184168595791562\n",
      "epoch 0 loss at step: 6110000 is 0.10964199537895619\n",
      "epoch 0 loss at step: 6115000 is 0.11387258453322574\n",
      "epoch 0 loss at step: 6120000 is 0.10669801707481966\n",
      "epoch 0 loss at step: 6125000 is 0.11200446526035666\n",
      "epoch 0 loss at step: 6130000 is 0.11139376256130636\n",
      "epoch 0 loss at step: 6135000 is 0.1122264206067659\n",
      "epoch 0 loss at step: 6140000 is 0.10979068097248673\n",
      "epoch 0 loss at step: 6145000 is 0.10842358629219234\n",
      "epoch 0 loss at step: 6150000 is 0.11055730137713253\n",
      "epoch 0 loss at step: 6155000 is 0.11383181600943208\n",
      "epoch 0 loss at step: 6160000 is 0.10521811712938361\n",
      "epoch 0 loss at step: 6165000 is 0.11237883567921818\n",
      "epoch 0 loss at step: 6170000 is 0.11023792176973074\n",
      "epoch 0 loss at step: 6175000 is 0.112503536959365\n",
      "epoch 0 loss at step: 6180000 is 0.11077842973228544\n",
      "epoch 0 loss at step: 6185000 is 0.1090859969664365\n",
      "epoch 0 loss at step: 6190000 is 0.10596095631998033\n",
      "epoch 0 loss at step: 6195000 is 0.1125263525177259\n",
      "epoch 0 loss at step: 6200000 is 0.10515613445648923\n",
      "epoch 0 loss at step: 6205000 is 0.10653975637368858\n",
      "epoch 0 loss at step: 6210000 is 0.10356084351893514\n",
      "epoch 0 loss at step: 6215000 is 0.10570048274751753\n",
      "epoch 0 loss at step: 6220000 is 0.11081501653697341\n",
      "epoch 0 loss at step: 6225000 is 0.10671747934669257\n",
      "epoch 0 loss at step: 6230000 is 0.11317763147614897\n",
      "epoch 0 loss at step: 6235000 is 0.10991665261313319\n",
      "epoch 0 loss at step: 6240000 is 0.1127071426043287\n",
      "epoch 0 loss at step: 6245000 is 0.10948563670972362\n",
      "epoch 0 loss at step: 6250000 is 0.1117749188398011\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "total_loss = 0\n",
    "plot_steps, print_steps = 5000, 5000\n",
    "step_cnt = 0\n",
    "all_losses_list = [] \n",
    "\n",
    "model.train() \n",
    "\n",
    "# Assuming you have a GPU available (cuda:0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "\n",
    "        # Move train_data to the GPU\n",
    "        train_data = {key: value.to(device) for key, value in train_data.items()}\n",
    "\n",
    "        output = model(train_data[\"users\"], train_data[\"animes\"])\n",
    "        \n",
    "        # .view(4, -1) is to reshape the rating to match the shape of model output which is 4x1\n",
    "        rating = train_data[\"ratings\"].view(4, -1).to(torch.float32).to(device)\n",
    "\n",
    "        loss = loss_func(output, rating)\n",
    "        total_loss = total_loss + loss.sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step_cnt = step_cnt + len(train_data[\"users\"])\n",
    "        \n",
    "        if(step_cnt % plot_steps == 0):\n",
    "            avg_loss = total_loss / (len(train_data[\"users\"]) * plot_steps)\n",
    "            print(f\"epoch {epoch_i} loss at step: {step_cnt} is {avg_loss}\")\n",
    "            all_losses_list.append(avg_loss)\n",
    "            total_loss = 0  # reset total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff322bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Your code for model and data loaders goes here...\n",
    "\n",
    "# Assuming you have a GPU available (cuda:0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# Number of epochs and steps to continue training\n",
    "epochs = 1\n",
    "total_steps_to_continue = 6250000  # Number of steps the model has already trained\n",
    "\n",
    "# Your code for other variables and setup...\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Load the model checkpoint if available\n",
    "try:\n",
    "    checkpoint = torch.load(\"model_checkpoint.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    step_cnt = checkpoint[\"step_cnt\"]\n",
    "    all_losses_list = checkpoint[\"all_losses_list\"]\n",
    "    print(\"Model checkpoint loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    step_cnt = 0\n",
    "    all_losses_list = []\n",
    "    print(\"No previous model checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        # Move train_data to the GPU\n",
    "        train_data = {key: value.to(device) for key, value in train_data.items()}\n",
    "\n",
    "        output = model(train_data[\"users\"], train_data[\"animes\"])\n",
    "        rating = train_data[\"ratings\"].view(4, -1).to(torch.float32).to(device)\n",
    "\n",
    "        loss = loss_func(output, rating)\n",
    "        total_loss = loss.sum().item()  # Calculate the loss per batch correctly\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step_cnt = step_cnt + len(train_data[\"users\"])\n",
    "\n",
    "        if step_cnt >= total_steps_to_continue and step_cnt % plot_steps == 0:\n",
    "            avg_loss = total_loss / (len(train_data[\"users\"]) * plot_steps)\n",
    "            print(f\"epoch {epoch_i} loss at step: {step_cnt} is {avg_loss}\")\n",
    "            all_losses_list.append(avg_loss)\n",
    "            total_loss = 0  # reset total_loss\n",
    "\n",
    "        # Save the model checkpoint after a certain number of steps\n",
    "        if step_cnt % total_steps_to_continue == 0:\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"step_cnt\": step_cnt,\n",
    "                \"all_losses_list\": all_losses_list\n",
    "            }, \"model_checkpoint.pth\")\n",
    "            print(\"Model checkpoint saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf2c6e-770f-4bae-8874-4700d7328215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
